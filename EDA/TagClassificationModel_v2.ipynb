{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tag Classification Model v2\n",
    "\n",
    "Author: \n",
    "* Luis Henrique M O Imagiire\n",
    "\n",
    "Status: \n",
    "* Done\n",
    "\n",
    "Goal:\n",
    "* This is a clearer developement from tagclassificationmodel notebook. Here we extend the first notebook by simplifying our model range (multiclassification -> binary) and using word position signal to increase model performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/luis/ds4a/notebooks\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have a total of 51655 reviews!\n"
     ]
    }
   ],
   "source": [
    "from src.loading import load_dataset\n",
    "from src.cleaning import build_df_from_RA\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gensim\n",
    "import unicodedata\n",
    "\n",
    "# Load dataFrame\n",
    "df = build_df_from_RA(load_dataset(\"dataset\"))\n",
    "print(f\"We have a total of {df.shape[0]} reviews!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import get_tmpfile\n",
    "from pathlib import Path\n",
    "from gensim.models.fasttext import FastText\n",
    "\n",
    "# Load model\n",
    "model = FastText.load(get_tmpfile(str(Path().cwd() / \"fasttext.model\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luis/ds4a/notebooks/src/model_inference.py:9: RuntimeWarning: Mean of empty slice.\n",
      "  try:\n",
      "/home/luis/anaconda3/envs/un_cuda/lib/python3.7/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feat_0</th>\n",
       "      <th>feat_1</th>\n",
       "      <th>feat_2</th>\n",
       "      <th>feat_3</th>\n",
       "      <th>feat_4</th>\n",
       "      <th>feat_5</th>\n",
       "      <th>feat_6</th>\n",
       "      <th>feat_7</th>\n",
       "      <th>feat_8</th>\n",
       "      <th>feat_9</th>\n",
       "      <th>...</th>\n",
       "      <th>feat_90</th>\n",
       "      <th>feat_91</th>\n",
       "      <th>feat_92</th>\n",
       "      <th>feat_93</th>\n",
       "      <th>feat_94</th>\n",
       "      <th>feat_95</th>\n",
       "      <th>feat_96</th>\n",
       "      <th>feat_97</th>\n",
       "      <th>feat_98</th>\n",
       "      <th>feat_99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.910237</td>\n",
       "      <td>0.948026</td>\n",
       "      <td>-0.580685</td>\n",
       "      <td>-1.087124</td>\n",
       "      <td>0.408073</td>\n",
       "      <td>0.035028</td>\n",
       "      <td>-0.255468</td>\n",
       "      <td>0.727001</td>\n",
       "      <td>0.511160</td>\n",
       "      <td>0.465338</td>\n",
       "      <td>...</td>\n",
       "      <td>0.225110</td>\n",
       "      <td>-0.556976</td>\n",
       "      <td>-0.199568</td>\n",
       "      <td>-0.029514</td>\n",
       "      <td>-0.384443</td>\n",
       "      <td>0.634457</td>\n",
       "      <td>-1.426050</td>\n",
       "      <td>-1.708523</td>\n",
       "      <td>0.028207</td>\n",
       "      <td>0.248345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.006765</td>\n",
       "      <td>0.400626</td>\n",
       "      <td>-0.188688</td>\n",
       "      <td>-2.611415</td>\n",
       "      <td>0.161582</td>\n",
       "      <td>1.001012</td>\n",
       "      <td>-1.393652</td>\n",
       "      <td>0.024986</td>\n",
       "      <td>-0.286601</td>\n",
       "      <td>0.050815</td>\n",
       "      <td>...</td>\n",
       "      <td>0.643617</td>\n",
       "      <td>-0.681656</td>\n",
       "      <td>-2.090754</td>\n",
       "      <td>0.766479</td>\n",
       "      <td>-0.360157</td>\n",
       "      <td>-0.899276</td>\n",
       "      <td>-0.045452</td>\n",
       "      <td>-1.029047</td>\n",
       "      <td>-0.453788</td>\n",
       "      <td>0.370695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.295514</td>\n",
       "      <td>-0.242192</td>\n",
       "      <td>-1.230961</td>\n",
       "      <td>-2.276299</td>\n",
       "      <td>-0.154886</td>\n",
       "      <td>0.099664</td>\n",
       "      <td>-1.411958</td>\n",
       "      <td>-0.046187</td>\n",
       "      <td>-0.566358</td>\n",
       "      <td>-0.810014</td>\n",
       "      <td>...</td>\n",
       "      <td>0.156349</td>\n",
       "      <td>-0.403661</td>\n",
       "      <td>-1.800049</td>\n",
       "      <td>0.152794</td>\n",
       "      <td>0.364936</td>\n",
       "      <td>-0.128633</td>\n",
       "      <td>-0.469290</td>\n",
       "      <td>-1.516815</td>\n",
       "      <td>0.014385</td>\n",
       "      <td>0.350531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.538521</td>\n",
       "      <td>-0.414169</td>\n",
       "      <td>-0.821009</td>\n",
       "      <td>-2.062811</td>\n",
       "      <td>-1.239713</td>\n",
       "      <td>0.311726</td>\n",
       "      <td>-0.904301</td>\n",
       "      <td>0.296257</td>\n",
       "      <td>0.148219</td>\n",
       "      <td>-1.170139</td>\n",
       "      <td>...</td>\n",
       "      <td>0.397926</td>\n",
       "      <td>-1.703135</td>\n",
       "      <td>-2.093588</td>\n",
       "      <td>0.467810</td>\n",
       "      <td>0.270992</td>\n",
       "      <td>-1.369555</td>\n",
       "      <td>-0.121953</td>\n",
       "      <td>-1.803416</td>\n",
       "      <td>0.848096</td>\n",
       "      <td>1.088491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.502684</td>\n",
       "      <td>0.101890</td>\n",
       "      <td>-0.161741</td>\n",
       "      <td>-2.076119</td>\n",
       "      <td>-0.018616</td>\n",
       "      <td>0.181313</td>\n",
       "      <td>-1.878871</td>\n",
       "      <td>-0.759554</td>\n",
       "      <td>-0.606767</td>\n",
       "      <td>-1.163182</td>\n",
       "      <td>...</td>\n",
       "      <td>0.418289</td>\n",
       "      <td>-1.644738</td>\n",
       "      <td>-2.112830</td>\n",
       "      <td>1.213724</td>\n",
       "      <td>0.711859</td>\n",
       "      <td>-0.254544</td>\n",
       "      <td>-1.367691</td>\n",
       "      <td>-1.562384</td>\n",
       "      <td>-0.460997</td>\n",
       "      <td>0.439617</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     feat_0    feat_1    feat_2    feat_3    feat_4    feat_5    feat_6  \\\n",
       "0  0.910237  0.948026 -0.580685 -1.087124  0.408073  0.035028 -0.255468   \n",
       "1 -0.006765  0.400626 -0.188688 -2.611415  0.161582  1.001012 -1.393652   \n",
       "2 -0.295514 -0.242192 -1.230961 -2.276299 -0.154886  0.099664 -1.411958   \n",
       "3  0.538521 -0.414169 -0.821009 -2.062811 -1.239713  0.311726 -0.904301   \n",
       "4  0.502684  0.101890 -0.161741 -2.076119 -0.018616  0.181313 -1.878871   \n",
       "\n",
       "     feat_7    feat_8    feat_9  ...   feat_90   feat_91   feat_92   feat_93  \\\n",
       "0  0.727001  0.511160  0.465338  ...  0.225110 -0.556976 -0.199568 -0.029514   \n",
       "1  0.024986 -0.286601  0.050815  ...  0.643617 -0.681656 -2.090754  0.766479   \n",
       "2 -0.046187 -0.566358 -0.810014  ...  0.156349 -0.403661 -1.800049  0.152794   \n",
       "3  0.296257  0.148219 -1.170139  ...  0.397926 -1.703135 -2.093588  0.467810   \n",
       "4 -0.759554 -0.606767 -1.163182  ...  0.418289 -1.644738 -2.112830  1.213724   \n",
       "\n",
       "    feat_94   feat_95   feat_96   feat_97   feat_98   feat_99  \n",
       "0 -0.384443  0.634457 -1.426050 -1.708523  0.028207  0.248345  \n",
       "1 -0.360157 -0.899276 -0.045452 -1.029047 -0.453788  0.370695  \n",
       "2  0.364936 -0.128633 -0.469290 -1.516815  0.014385  0.350531  \n",
       "3  0.270992 -1.369555 -0.121953 -1.803416  0.848096  1.088491  \n",
       "4  0.711859 -0.254544 -1.367691 -1.562384 -0.460997  0.439617  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.model_inference import get_text_feature_df\n",
    "\n",
    "# Get feature df\n",
    "feat_df = get_text_feature_df(df, model)\n",
    "feat_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_issues_Quality</th>\n",
       "      <th>product_issues_Damaged</th>\n",
       "      <th>product_issues_Electrical problems</th>\n",
       "      <th>product_issues_Missing pieces</th>\n",
       "      <th>business_issues_Payment</th>\n",
       "      <th>business_issues_Maintenance</th>\n",
       "      <th>business_issues_Customer Services</th>\n",
       "      <th>business_issues_Delivery</th>\n",
       "      <th>business_issues_Online Services</th>\n",
       "      <th>product_issue</th>\n",
       "      <th>business_issue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   product_issues_Quality  product_issues_Damaged  \\\n",
       "0                       1                       0   \n",
       "1                       0                       0   \n",
       "2                       1                       0   \n",
       "4                       0                       0   \n",
       "5                       1                       0   \n",
       "\n",
       "   product_issues_Electrical problems  product_issues_Missing pieces  \\\n",
       "0                                   0                              0   \n",
       "1                                   0                              0   \n",
       "2                                   0                              0   \n",
       "4                                   0                              0   \n",
       "5                                   0                              0   \n",
       "\n",
       "   business_issues_Payment  business_issues_Maintenance  \\\n",
       "0                        0                            0   \n",
       "1                        0                            0   \n",
       "2                        0                            0   \n",
       "4                        0                            0   \n",
       "5                        0                            0   \n",
       "\n",
       "   business_issues_Customer Services  business_issues_Delivery  \\\n",
       "0                                  0                         0   \n",
       "1                                  1                         1   \n",
       "2                                  0                         0   \n",
       "4                                  1                         0   \n",
       "5                                  0                         0   \n",
       "\n",
       "   business_issues_Online Services  product_issue  business_issue  \n",
       "0                                0              1               0  \n",
       "1                                0              0               1  \n",
       "2                                0              1               0  \n",
       "4                                1              0               1  \n",
       "5                                0              1               0  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pre-load tag columns\n",
    "product_issues_columns = ['product_issues_Quality','product_issues_Damaged', 'product_issues_Electrical problems','product_issues_Missing pieces']\n",
    "business_issues_columns = ['business_issues_Payment','business_issues_Maintenance', 'business_issues_Customer Services','business_issues_Delivery', 'business_issues_Online Services']\n",
    "tag_columns = product_issues_columns.copy()\n",
    "tag_columns.extend(business_issues_columns)\n",
    "\n",
    "# Create product/business issue tag and drop those who have both tagged to simplify our analysis\n",
    "df_tag = df[tag_columns].fillna(0).applymap(lambda x: 1 if x>0 else 0)\n",
    "df_tag[\"product_issue\"] = df_tag[product_issues_columns].max(axis=1)\n",
    "df_tag[\"business_issue\"] = df_tag[business_issues_columns].max(axis=1)\n",
    "df_tag = df_tag[df_tag[[\"product_issue\", \"business_issue\"]].sum(axis=1) != 2]\n",
    "df_tag = df_tag[df_tag[[\"product_issue\", \"business_issue\"]].sum(axis=1) != 0]\n",
    "\n",
    "df_tag = df_tag.loc[df_tag.index.intersection(feat_df.index)] # match index from feature df \n",
    "df_tag.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = feat_df.loc[feat_df.index.intersection(df_tag.index)]\n",
    "Y = df_tag\n",
    "\n",
    "X_test = X.iloc[-100::]\n",
    "Y_test = Y.iloc[-100::]\n",
    "\n",
    "X_t = X.iloc[0:-100]\n",
    "Y_t = Y.iloc[0:-100]\n",
    "\n",
    "# Sanity check\n",
    "assert X_test.shape[0] + X_t.shape[0] == X.shape[0]\n",
    "assert Y_test.shape[0] + Y_t.shape[0] == Y.shape[0]\n",
    "\n",
    "seed = 2020\n",
    "split = 0.95  # We have a bunch of data\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X_t, Y_t, train_size=split, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_issues_Quality</th>\n",
       "      <th>product_issues_Damaged</th>\n",
       "      <th>product_issues_Electrical problems</th>\n",
       "      <th>product_issues_Missing pieces</th>\n",
       "      <th>business_issues_Payment</th>\n",
       "      <th>business_issues_Maintenance</th>\n",
       "      <th>business_issues_Customer Services</th>\n",
       "      <th>business_issues_Delivery</th>\n",
       "      <th>business_issues_Online Services</th>\n",
       "      <th>product_issue</th>\n",
       "      <th>business_issue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23095</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42575</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16495</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14998</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28939</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       product_issues_Quality  product_issues_Damaged  \\\n",
       "23095                       0                       1   \n",
       "42575                       0                       0   \n",
       "16495                       0                       0   \n",
       "14998                       0                       1   \n",
       "28939                       0                       0   \n",
       "\n",
       "       product_issues_Electrical problems  product_issues_Missing pieces  \\\n",
       "23095                                   0                              0   \n",
       "42575                                   0                              0   \n",
       "16495                                   0                              0   \n",
       "14998                                   0                              0   \n",
       "28939                                   0                              0   \n",
       "\n",
       "       business_issues_Payment  business_issues_Maintenance  \\\n",
       "23095                        0                            0   \n",
       "42575                        1                            0   \n",
       "16495                        0                            0   \n",
       "14998                        0                            0   \n",
       "28939                        0                            0   \n",
       "\n",
       "       business_issues_Customer Services  business_issues_Delivery  \\\n",
       "23095                                  0                         0   \n",
       "42575                                  0                         0   \n",
       "16495                                  1                         0   \n",
       "14998                                  0                         0   \n",
       "28939                                  0                         1   \n",
       "\n",
       "       business_issues_Online Services  product_issue  business_issue  \n",
       "23095                                0              1               0  \n",
       "42575                                0              0               1  \n",
       "16495                                0              0               1  \n",
       "14998                                0              1               0  \n",
       "28939                                0              0               1  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luis/anaconda3/envs/un_cuda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/luis/anaconda3/envs/un_cuda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/luis/anaconda3/envs/un_cuda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/luis/anaconda3/envs/un_cuda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/luis/anaconda3/envs/un_cuda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/luis/anaconda3/envs/un_cuda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/luis/anaconda3/envs/un_cuda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/luis/anaconda3/envs/un_cuda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/luis/anaconda3/envs/un_cuda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/luis/anaconda3/envs/un_cuda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/luis/anaconda3/envs/un_cuda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/luis/anaconda3/envs/un_cuda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "tf.config.experimental.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 24671 samples, validate on 1299 samples\n",
      "Epoch 1/40\n",
      "24320/24671 [============================>.] - ETA: 0s - loss: 0.3219 - acc: 0.8574\n",
      "Epoch 00001: val_loss improved from -inf to 0.24240, saving model to weights-improvement-BinaryModel-01-0.24.hdf5\n",
      "24671/24671 [==============================] - 3s 129us/sample - loss: 0.3218 - acc: 0.8574 - val_loss: 0.2424 - val_acc: 0.8807\n",
      "Epoch 2/40\n",
      "24320/24671 [============================>.] - ETA: 0s - loss: 0.2691 - acc: 0.8833\n",
      "Epoch 00002: val_loss did not improve from 0.24240\n",
      "24671/24671 [==============================] - 2s 91us/sample - loss: 0.2692 - acc: 0.8834 - val_loss: 0.2348 - val_acc: 0.8807\n",
      "Epoch 3/40\n",
      "24448/24671 [============================>.] - ETA: 0s - loss: 0.2661 - acc: 0.8874\n",
      "Epoch 00003: val_loss did not improve from 0.24240\n",
      "24671/24671 [==============================] - 2s 92us/sample - loss: 0.2654 - acc: 0.8878 - val_loss: 0.2385 - val_acc: 0.8876\n",
      "Epoch 4/40\n",
      "24448/24671 [============================>.] - ETA: 0s - loss: 0.2572 - acc: 0.8916\n",
      "Epoch 00004: val_loss did not improve from 0.24240\n",
      "24671/24671 [==============================] - 2s 92us/sample - loss: 0.2570 - acc: 0.8917 - val_loss: 0.2289 - val_acc: 0.8876\n",
      "Epoch 5/40\n",
      "24320/24671 [============================>.] - ETA: 0s - loss: 0.2545 - acc: 0.8909\n",
      "Epoch 00005: val_loss did not improve from 0.24240\n",
      "24671/24671 [==============================] - 2s 94us/sample - loss: 0.2545 - acc: 0.8910 - val_loss: 0.2296 - val_acc: 0.8930\n",
      "Epoch 6/40\n",
      "24320/24671 [============================>.] - ETA: 0s - loss: 0.2471 - acc: 0.8930\n",
      "Epoch 00006: val_loss improved from 0.24240 to 0.24493, saving model to weights-improvement-BinaryModel-06-0.24.hdf5\n",
      "24671/24671 [==============================] - 2s 98us/sample - loss: 0.2468 - acc: 0.8931 - val_loss: 0.2449 - val_acc: 0.8868\n",
      "Epoch 7/40\n",
      "24192/24671 [============================>.] - ETA: 0s - loss: 0.2547 - acc: 0.8910\n",
      "Epoch 00007: val_loss did not improve from 0.24493\n",
      "24671/24671 [==============================] - 2s 93us/sample - loss: 0.2546 - acc: 0.8910 - val_loss: 0.2295 - val_acc: 0.8922\n",
      "Epoch 8/40\n",
      "24448/24671 [============================>.] - ETA: 0s - loss: 0.2480 - acc: 0.8938\n",
      "Epoch 00008: val_loss did not improve from 0.24493\n",
      "24671/24671 [==============================] - 2s 93us/sample - loss: 0.2478 - acc: 0.8938 - val_loss: 0.2317 - val_acc: 0.8915\n",
      "Epoch 9/40\n",
      "24448/24671 [============================>.] - ETA: 0s - loss: 0.2427 - acc: 0.8957\n",
      "Epoch 00009: val_loss did not improve from 0.24493\n",
      "24671/24671 [==============================] - 2s 93us/sample - loss: 0.2427 - acc: 0.8958 - val_loss: 0.2266 - val_acc: 0.8984\n",
      "Epoch 10/40\n",
      "24448/24671 [============================>.] - ETA: 0s - loss: 0.2418 - acc: 0.8955\n",
      "Epoch 00010: val_loss did not improve from 0.24493\n",
      "24671/24671 [==============================] - 2s 95us/sample - loss: 0.2420 - acc: 0.8955 - val_loss: 0.2274 - val_acc: 0.8915\n",
      "Epoch 11/40\n",
      "24192/24671 [============================>.] - ETA: 0s - loss: 0.2358 - acc: 0.8999\n",
      "Epoch 00011: val_loss did not improve from 0.24493\n",
      "24671/24671 [==============================] - 2s 95us/sample - loss: 0.2362 - acc: 0.8998 - val_loss: 0.2314 - val_acc: 0.8876\n",
      "Epoch 12/40\n",
      "24064/24671 [============================>.] - ETA: 0s - loss: 0.2381 - acc: 0.8981\n",
      "Epoch 00012: val_loss did not improve from 0.24493\n",
      "24671/24671 [==============================] - 2s 95us/sample - loss: 0.2384 - acc: 0.8979 - val_loss: 0.2364 - val_acc: 0.8915\n",
      "Epoch 13/40\n",
      "24192/24671 [============================>.] - ETA: 0s - loss: 0.2376 - acc: 0.8998\n",
      "Epoch 00013: val_loss did not improve from 0.24493\n",
      "24671/24671 [==============================] - 2s 94us/sample - loss: 0.2386 - acc: 0.8996 - val_loss: 0.2205 - val_acc: 0.8915\n",
      "Epoch 14/40\n",
      "24448/24671 [============================>.] - ETA: 0s - loss: 0.2357 - acc: 0.9004\n",
      "Epoch 00014: val_loss did not improve from 0.24493\n",
      "24671/24671 [==============================] - 2s 96us/sample - loss: 0.2353 - acc: 0.9008 - val_loss: 0.2322 - val_acc: 0.8945\n",
      "Epoch 15/40\n",
      "24192/24671 [============================>.] - ETA: 0s - loss: 0.2276 - acc: 0.9020\n",
      "Epoch 00015: val_loss did not improve from 0.24493\n",
      "24671/24671 [==============================] - 2s 97us/sample - loss: 0.2285 - acc: 0.9016 - val_loss: 0.2262 - val_acc: 0.8930\n",
      "Epoch 16/40\n",
      "24192/24671 [============================>.] - ETA: 0s - loss: 0.2282 - acc: 0.9038\n",
      "Epoch 00016: val_loss did not improve from 0.24493\n",
      "24671/24671 [==============================] - 2s 95us/sample - loss: 0.2282 - acc: 0.9039 - val_loss: 0.2377 - val_acc: 0.8938\n",
      "Epoch 17/40\n",
      "24320/24671 [============================>.] - ETA: 0s - loss: 0.2308 - acc: 0.9024\n",
      "Epoch 00017: val_loss did not improve from 0.24493\n",
      "24671/24671 [==============================] - 2s 94us/sample - loss: 0.2314 - acc: 0.9021 - val_loss: 0.2413 - val_acc: 0.9030\n",
      "Epoch 18/40\n",
      "24320/24671 [============================>.] - ETA: 0s - loss: 0.2299 - acc: 0.9035\n",
      "Epoch 00018: val_loss improved from 0.24493 to 0.24878, saving model to weights-improvement-BinaryModel-18-0.25.hdf5\n",
      "24671/24671 [==============================] - 2s 99us/sample - loss: 0.2295 - acc: 0.9035 - val_loss: 0.2488 - val_acc: 0.8814\n",
      "Epoch 19/40\n",
      "24192/24671 [============================>.] - ETA: 0s - loss: 0.2315 - acc: 0.9036\n",
      "Epoch 00019: val_loss did not improve from 0.24878\n",
      "24671/24671 [==============================] - 2s 93us/sample - loss: 0.2317 - acc: 0.9036 - val_loss: 0.2304 - val_acc: 0.9007\n",
      "Epoch 20/40\n",
      "24448/24671 [============================>.] - ETA: 0s - loss: 0.2234 - acc: 0.9059\n",
      "Epoch 00020: val_loss did not improve from 0.24878\n",
      "24671/24671 [==============================] - 2s 92us/sample - loss: 0.2230 - acc: 0.9060 - val_loss: 0.2484 - val_acc: 0.8922\n",
      "Epoch 21/40\n",
      "24448/24671 [============================>.] - ETA: 0s - loss: 0.2205 - acc: 0.9075\n",
      "Epoch 00021: val_loss improved from 0.24878 to 0.25160, saving model to weights-improvement-BinaryModel-21-0.25.hdf5\n",
      "24671/24671 [==============================] - 2s 98us/sample - loss: 0.2211 - acc: 0.9070 - val_loss: 0.2516 - val_acc: 0.9045\n",
      "Epoch 22/40\n",
      "24448/24671 [============================>.] - ETA: 0s - loss: 0.2239 - acc: 0.9063\n",
      "Epoch 00022: val_loss did not improve from 0.25160\n",
      "24671/24671 [==============================] - 2s 95us/sample - loss: 0.2234 - acc: 0.9066 - val_loss: 0.2404 - val_acc: 0.9053\n",
      "Epoch 23/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.2179 - acc: 0.9086\n",
      "Epoch 00023: val_loss did not improve from 0.25160\n",
      "24671/24671 [==============================] - 2s 96us/sample - loss: 0.2178 - acc: 0.9086 - val_loss: 0.2381 - val_acc: 0.8938\n",
      "Epoch 24/40\n",
      "24448/24671 [============================>.] - ETA: 0s - loss: 0.2191 - acc: 0.9058\n",
      "Epoch 00024: val_loss improved from 0.25160 to 0.26371, saving model to weights-improvement-BinaryModel-24-0.26.hdf5\n",
      "24671/24671 [==============================] - 2s 101us/sample - loss: 0.2194 - acc: 0.9059 - val_loss: 0.2637 - val_acc: 0.8976\n",
      "Epoch 25/40\n",
      "24064/24671 [============================>.] - ETA: 0s - loss: 0.2165 - acc: 0.9094\n",
      "Epoch 00025: val_loss did not improve from 0.26371\n",
      "24671/24671 [==============================] - 2s 95us/sample - loss: 0.2166 - acc: 0.9093 - val_loss: 0.2423 - val_acc: 0.8976\n",
      "Epoch 26/40\n",
      "24448/24671 [============================>.] - ETA: 0s - loss: 0.2143 - acc: 0.9096\n",
      "Epoch 00026: val_loss did not improve from 0.26371\n",
      "24671/24671 [==============================] - 2s 96us/sample - loss: 0.2144 - acc: 0.9097 - val_loss: 0.2356 - val_acc: 0.8930\n",
      "Epoch 27/40\n",
      "24064/24671 [============================>.] - ETA: 0s - loss: 0.2088 - acc: 0.9085\n",
      "Epoch 00027: val_loss improved from 0.26371 to 0.27688, saving model to weights-improvement-BinaryModel-27-0.28.hdf5\n",
      "24671/24671 [==============================] - 2s 99us/sample - loss: 0.2091 - acc: 0.9088 - val_loss: 0.2769 - val_acc: 0.9015\n",
      "Epoch 28/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.2123 - acc: 0.9113\n",
      "Epoch 00028: val_loss did not improve from 0.27688\n",
      "24671/24671 [==============================] - 2s 94us/sample - loss: 0.2124 - acc: 0.9112 - val_loss: 0.2541 - val_acc: 0.8961\n",
      "Epoch 29/40\n",
      "24448/24671 [============================>.] - ETA: 0s - loss: 0.2058 - acc: 0.9104\n",
      "Epoch 00029: val_loss did not improve from 0.27688\n",
      "24671/24671 [==============================] - 2s 95us/sample - loss: 0.2054 - acc: 0.9105 - val_loss: 0.2606 - val_acc: 0.8999\n",
      "Epoch 30/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.2107 - acc: 0.9086\n",
      "Epoch 00030: val_loss did not improve from 0.27688\n",
      "24671/24671 [==============================] - 2s 97us/sample - loss: 0.2106 - acc: 0.9088 - val_loss: 0.2434 - val_acc: 0.8961\n",
      "Epoch 31/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.2032 - acc: 0.9142\n",
      "Epoch 00031: val_loss did not improve from 0.27688\n",
      "24671/24671 [==============================] - 2s 98us/sample - loss: 0.2031 - acc: 0.9142 - val_loss: 0.2557 - val_acc: 0.8999\n",
      "Epoch 32/40\n",
      "24064/24671 [============================>.] - ETA: 0s - loss: 0.2038 - acc: 0.9143\n",
      "Epoch 00032: val_loss did not improve from 0.27688\n",
      "24671/24671 [==============================] - 2s 94us/sample - loss: 0.2035 - acc: 0.9145 - val_loss: 0.2570 - val_acc: 0.9007\n",
      "Epoch 33/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.1999 - acc: 0.9139\n",
      "Epoch 00033: val_loss did not improve from 0.27688\n",
      "24671/24671 [==============================] - 2s 96us/sample - loss: 0.1997 - acc: 0.9140 - val_loss: 0.2704 - val_acc: 0.8984\n",
      "Epoch 34/40\n",
      "24064/24671 [============================>.] - ETA: 0s - loss: 0.1982 - acc: 0.9163\n",
      "Epoch 00034: val_loss improved from 0.27688 to 0.27709, saving model to weights-improvement-BinaryModel-34-0.28.hdf5\n",
      "24671/24671 [==============================] - 3s 102us/sample - loss: 0.1979 - acc: 0.9163 - val_loss: 0.2771 - val_acc: 0.9076\n",
      "Epoch 35/40\n",
      "24320/24671 [============================>.] - ETA: 0s - loss: 0.2015 - acc: 0.9153\n",
      "Epoch 00035: val_loss did not improve from 0.27709\n",
      "24671/24671 [==============================] - 2s 93us/sample - loss: 0.2019 - acc: 0.9152 - val_loss: 0.2579 - val_acc: 0.8884\n",
      "Epoch 36/40\n",
      "24064/24671 [============================>.] - ETA: 0s - loss: 0.1998 - acc: 0.9157\n",
      "Epoch 00036: val_loss did not improve from 0.27709\n",
      "24671/24671 [==============================] - 2s 95us/sample - loss: 0.1996 - acc: 0.9160 - val_loss: 0.2678 - val_acc: 0.8938\n",
      "Epoch 37/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.1957 - acc: 0.9169\n",
      "Epoch 00037: val_loss did not improve from 0.27709\n",
      "24671/24671 [==============================] - 2s 97us/sample - loss: 0.1957 - acc: 0.9170 - val_loss: 0.2698 - val_acc: 0.9038\n",
      "Epoch 38/40\n",
      "24064/24671 [============================>.] - ETA: 0s - loss: 0.1987 - acc: 0.9165\n",
      "Epoch 00038: val_loss did not improve from 0.27709\n",
      "24671/24671 [==============================] - 2s 94us/sample - loss: 0.1982 - acc: 0.9168 - val_loss: 0.2663 - val_acc: 0.9045\n",
      "Epoch 39/40\n",
      "24064/24671 [============================>.] - ETA: 0s - loss: 0.1961 - acc: 0.9167\n",
      "Epoch 00039: val_loss did not improve from 0.27709\n",
      "24671/24671 [==============================] - 2s 95us/sample - loss: 0.1961 - acc: 0.9166 - val_loss: 0.2506 - val_acc: 0.9015\n",
      "Epoch 40/40\n",
      "24064/24671 [============================>.] - ETA: 0s - loss: 0.1978 - acc: 0.9173\n",
      "Epoch 00040: val_loss did not improve from 0.27709\n",
      "24671/24671 [==============================] - 2s 93us/sample - loss: 0.1979 - acc: 0.9172 - val_loss: 0.2551 - val_acc: 0.8930\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Pre-loading\n",
    "filepath=\"weights-improvement-BinaryModel-{epoch:02d}-{val_loss:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "embedding_dim = 100\n",
    "batch_size = 128\n",
    "epochs = 40\n",
    "stack = 6\n",
    "\n",
    "\n",
    "def create_network(n_dense=6,\n",
    "                   dense_units=16,\n",
    "                   activation='selu',\n",
    "                   dropout=layers.Dropout,\n",
    "                   dropout_rate=0.1,\n",
    "                   kernel_initializer='lecun_normal',\n",
    "                   optimizer='adam',\n",
    "                   num_classes=1,\n",
    "                   max_words=embedding_dim):\n",
    "    \n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(layers.Dense(dense_units, input_shape=(max_words,),\n",
    "                    kernel_initializer=kernel_initializer))\n",
    "    model.add(layers.Activation(activation))\n",
    "    model.add(dropout(dropout_rate))\n",
    "\n",
    "    for i in range(n_dense - 1):\n",
    "        model.add(layers.Dense(dense_units//(2+i), kernel_initializer=kernel_initializer))\n",
    "        model.add(layers.Activation(activation))\n",
    "        model.add(dropout(dropout_rate))\n",
    "\n",
    "    model.add(layers.Dense(1))\n",
    "    model.add(layers.Activation('sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "network1 = {\n",
    "    'n_dense': stack,\n",
    "    'dense_units': 4096,\n",
    "    'activation': 'relu',\n",
    "    'dropout': layers.Dropout,\n",
    "    'dropout_rate': 0.5,\n",
    "    'kernel_initializer': 'glorot_uniform',\n",
    "    'optimizer': 'adam'\n",
    "}\n",
    "\n",
    "model1 = create_network(num_classes=num_classes, **network1)\n",
    "history_model1 = model1.fit(X_train,\n",
    "                            y_train[\"product_issue\"].values.reshape(-1,1),\n",
    "                            batch_size=batch_size,\n",
    "                            epochs=epochs,\n",
    "                            verbose=1,\n",
    "                            callbacks=callbacks_list,\n",
    "                            validation_data=(X_validation, y_validation[\"product_issue\"].values.reshape(-1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 1, 1, 0]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_predictions(dataset, class_model, decision=0.5):\n",
    "    decision_rule = lambda x: 1 if x>=decision else 0\n",
    "    predicts = [decision_rule(i) for i in model1.predict(dataset)]\n",
    "    return predicts\n",
    "\n",
    "predicts = get_predictions(X_validation, model1, decision=0.5)\n",
    "predicts[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[938,  89],\n",
       "       [ 50, 222]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_validation[\"product_issue\"].values.reshape(-1,1), np.array(predicts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(fpr_keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x7f3344c0b8d0>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "# ROC\n",
    "fpr_keras, tpr_keras, thresholds_keras = roc_curve(y_validation[\"product_issue\"].values.reshape(-1,1), model1.predict(X_validation).ravel())\n",
    "sns.relplot(x=\"FPR\", y=\"TPR\", data=pd.DataFrame(np.array([fpr_keras, tpr_keras]).transpose(), columns=[\"FPR\", \"TPR\"]), kind=\"line\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remarks\n",
    "\n",
    "The model has a good general performance. We will tag GB dataset to enable further EDA.\n",
    "Lets proceed by doing a RNN variation to account for word position and see if we have improvements.\n",
    "\n",
    "Next, we proceed by making a model for each subset (business or product issues) and incrementing with RNNs as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have a total of 140416 reviews!\n",
      "{'caption': '',\n",
      " 'geo_location': {'lat': '-23.5975251', 'long': '-46.6025457'},\n",
      " 'id_review': 'ChZDSUhNMG9nS0VJQ0FnSUNjemR6YkRREAEaFkNKMExMS0tRQnA3NmhQUzlCWkoyTkE',\n",
      " 'n_photo_user': 0,\n",
      " 'n_review_user': 1,\n",
      " 'rating': 4.0,\n",
      " 'relative_date': '3 semanas atrÃ¡s',\n",
      " 'retrieval_date': '2020-04-15T05:32:36Z',\n",
      " 'store': 'magazine-luiza',\n",
      " 'url_user': 'https://www.google.com/maps/contrib/114096832037584768938?hl=pt-BR',\n",
      " 'username': 'Tiago Leonel Barreto'}\n"
     ]
    }
   ],
   "source": [
    "from EDA.util import agg_jsonls  # helper function\n",
    "from pprint import pprint\n",
    "\n",
    "def load_unique_GB(dataset_folder):\n",
    "    unique_ids = set()\n",
    "    reviewsGB = []\n",
    "    for complaint in agg_jsonls(dataset_folder):\n",
    "        if \"id_review\" in complaint and complaint[\"id_review\"] not in unique_ids:\n",
    "            unique_ids.add(complaint[\"id_review\"])\n",
    "            reviewsGB.append(complaint)\n",
    "    return reviewsGB\n",
    "\n",
    "reviewsGB = load_unique_GB(\"final_dataset_GB_1704\")\n",
    "print(f\"We have a total of {len(reviewsGB)} reviews!\")\n",
    "pprint(reviewsGB[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luis/ds4a/notebooks/src/model_inference.py:9: RuntimeWarning: Mean of empty slice.\n",
      "  try:\n",
      "/home/luis/anaconda3/envs/un_cuda/lib/python3.7/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "from src.text_formatting import remove_numbers, normalize_text,tokenize\n",
    "from src.model_inference import get_review_embbedings\n",
    "\n",
    "def get_tokens_from_GB(dataset):\n",
    "    def format_text_input(row):\n",
    "        return f\"{remove_numbers(normalize_text(row['caption']))}\"\n",
    "    \n",
    "    texts = [format_text_input(r) for r in dataset]\n",
    "    tokens_sq = [tokenize(i) for i in texts]\n",
    "    return tokens_sq\n",
    "\n",
    "neg_reviews = [r for r in reviewsGB if r[\"rating\"] <3 and len(r[\"caption\"])>0]\n",
    "gb_feat_vecs = [get_review_embbedings(token, model) for token in get_tokens_from_GB([r for r in neg_reviews])]\n",
    "feat_num = gb_feat_vecs[0].shape[0]\n",
    "indexes = pd.Series(gb_feat_vecs)\n",
    "indexes = indexes[~indexes.isna()].index\n",
    "\n",
    "# Remove nans and maintain index order to df\n",
    "gb_df_feat = pd.DataFrame([i for i in gb_feat_vecs if not isinstance(i, type(np.nan))], columns=[f\"feat_{i}\" for i in range(feat_num)], index=indexes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.822335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0\n",
       "0  0.822335\n",
       "1  1.000000\n",
       "2  0.000000\n",
       "3  0.001313\n",
       "4  0.000000"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb_preds = pd.DataFrame(model1.predict(gb_df_feat), index=indexes)\n",
    "gb_preds.columns = [\"Issue\"]\n",
    "gb_preds.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tag reviews\n",
    "for idx,r in enumerate(neg_reviews):\n",
    "    if idx in gb_preds.index:\n",
    "        r[\"issue\"] = gb_preds.loc[idx].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id_review': 'ChZDSUhNMG9nS0VJQ0FnSURNNXBERUFREAEaFjZwVEFadG8xblgySHZTdEdrMEJlYmc',\n",
       "  'caption': 'Vende eletros domÃ©sticos quebrado.  Comprei um liquidificador com defeito.',\n",
       "  'relative_date': '3 meses atrÃ¡s',\n",
       "  'retrieval_date': '2020-04-15T05:33:14Z',\n",
       "  'rating': 2.0,\n",
       "  'username': 'Mercia Oliveira',\n",
       "  'n_review_user': 39,\n",
       "  'n_photo_user': 0,\n",
       "  'url_user': 'https://www.google.com/maps/contrib/107839935361960381477?hl=pt-BR',\n",
       "  'geo_location': {'lat': '-23.5975251', 'long': '-46.6025457'},\n",
       "  'store': 'magazine-luiza',\n",
       "  'issue': 'product_issue'},\n",
       " {'id_review': 'ChdDSUhNMG9nS0VJQ0FnSUNNdjhMWHF3RRABGhZ2bHdkUXRxdEJkWjlEMEVlclh5R3FR',\n",
       "  'caption': 'Som muito alto',\n",
       "  'relative_date': '4 meses atrÃ¡s',\n",
       "  'retrieval_date': '2020-04-15T05:33:21Z',\n",
       "  'rating': 1.0,\n",
       "  'username': 'Wendy Castiello',\n",
       "  'n_review_user': 0,\n",
       "  'n_photo_user': 0,\n",
       "  'url_user': 'https://www.google.com/maps/contrib/109623549828345753675?hl=pt-BR',\n",
       "  'geo_location': {'lat': '-23.5975251', 'long': '-46.6025457'},\n",
       "  'store': 'magazine-luiza',\n",
       "  'issue': 'product_issue'},\n",
       " {'id_review': 'ChZDSUhNMG9nS0VJQ0FnSUNNdDVidE5nEAEaFmE3R1pMTGtPdmJIUU9tQXYxd2xqVGc',\n",
       "  'caption': 'PÃ©ssimo atendimento e nÃ£o tem telefone para atender os clientes tem que ir na loja minha primeira compra e Ãºltima nÃ£o dÃ£o satisfaÃ§Ã£o da entrega pqp',\n",
       "  'relative_date': '4 meses atrÃ¡s',\n",
       "  'retrieval_date': '2020-04-15T05:33:21Z',\n",
       "  'rating': 1.0,\n",
       "  'username': 'Gilberto Alves',\n",
       "  'n_review_user': 14,\n",
       "  'n_photo_user': 0,\n",
       "  'url_user': 'https://www.google.com/maps/contrib/113640391183338402739?hl=pt-BR',\n",
       "  'geo_location': {'lat': '-23.5975251', 'long': '-46.6025457'},\n",
       "  'store': 'magazine-luiza',\n",
       "  'issue': 'business_issue'},\n",
       " {'id_review': 'ChdDSUhNMG9nS0VJQ0FnSUQwdE5QTzVRRRABGhZVOThjbXNEdHBialgwU1JFN3EyQTZB',\n",
       "  'caption': 'PÃ©ssima..  Pessoas descompromissadas...me impediram do meu direito.de retirar meu televisor pago...simplesmente fecharam a loja ANTES do horÃ¡rio anunciado ...e ainda tiveram o desplante de dizer que era Normal e ainda fui.ameacado de processo pela funcionÃ¡ria do caixa que foi extremamente deselegante e sem educaÃ§Ã£o.  Tenho documentado os nomes de todos estes atores e jÃ¡ descobri o nome do supervisor regional. Fujam desta empresa',\n",
       "  'relative_date': '5 meses atrÃ¡s',\n",
       "  'retrieval_date': '2020-04-15T05:33:41Z',\n",
       "  'rating': 1.0,\n",
       "  'username': 'Anderson Freire Carniel',\n",
       "  'n_review_user': 1,\n",
       "  'n_photo_user': 0,\n",
       "  'url_user': 'https://www.google.com/maps/contrib/107424083193977237669?hl=pt-BR',\n",
       "  'geo_location': {'lat': '-23.5975251', 'long': '-46.6025457'},\n",
       "  'store': 'magazine-luiza',\n",
       "  'issue': 'business_issue'},\n",
       " {'id_review': 'ChdDSUhNMG9nS0VJQ0FnSUQwa083dHdnRRABGhYwY000cjh6OUtqZGE5OG1KeVRmbk5n',\n",
       "  'caption': 'Bom atendimento.',\n",
       "  'relative_date': '5 meses atrÃ¡s',\n",
       "  'retrieval_date': '2020-04-15T05:33:41Z',\n",
       "  'rating': 1.0,\n",
       "  'username': 'Helena Stauber',\n",
       "  'n_review_user': 0,\n",
       "  'n_photo_user': 0,\n",
       "  'url_user': 'https://www.google.com/maps/contrib/117657327781010046061?hl=pt-BR',\n",
       "  'geo_location': {'lat': '-23.5975251', 'long': '-46.6025457'},\n",
       "  'store': 'magazine-luiza',\n",
       "  'issue': 'business_issue'}]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate model predictions\n",
    "neg_reviews[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Dump our enriched dataset\n",
    "with open(\"GB_tagged_bin.jsonl\", \"w\", encoding=\"utf-8\") as js:\n",
    "    for r in neg_reviews:\n",
    "        js.write(json.dumps(r) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding RNN layer \n",
    "\n",
    "We will need to establish a sequence lenght and pad sequences that are lower than seq lenght."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "neg_reviews_tokens_len = pd.Series([len(r) for r in get_tokens_from_GB(neg_reviews)])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "\n",
    "# plot the cumulative histogram\n",
    "n, bins, patches = ax.hist([i for i in neg_reviews_tokens_len if i <100], 80, density=True, histtype='step',cumulative=True, label='Empirical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets set our sequence pad to 25 as a starting point. We re-evaluate it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = np.zeros([25,model.vector_size])\n",
    "seq[0,:] = np.ones(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.text_formatting import get_tokens_from_RA_df\n",
    "\n",
    "def get_review_embbedings_seq(review_seq, model: \"gensim.model\", seq_max=25):\n",
    "    final_seq = np.zeros([seq_max,model.vector_size])\n",
    "    for i,w in enumerate(review_seq):\n",
    "        if i<seq_max:\n",
    "            try:\n",
    "                final_seq[i,:] = model.wv[w]\n",
    "            except Exception as err:\n",
    "                print(err)\n",
    "    return final_seq\n",
    "\n",
    "max_seq = 25\n",
    "# toks = get_tokens_from_GB(neg_reviews)\n",
    "toks = get_tokens_from_RA_df(df)\n",
    "feat_seq = np.array([get_review_embbedings_seq(token, model, seq_max=25) for token in toks])\n",
    "\n",
    "# Sanity check\n",
    "assert feat_seq.shape == (len(toks), max_seq, model.vector_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-load tag columns\n",
    "product_issues_columns = ['product_issues_Quality','product_issues_Damaged', 'product_issues_Electrical problems','product_issues_Missing pieces']\n",
    "business_issues_columns = ['business_issues_Payment','business_issues_Maintenance', 'business_issues_Customer Services','business_issues_Delivery', 'business_issues_Online Services']\n",
    "tag_columns = product_issues_columns.copy()\n",
    "tag_columns.extend(business_issues_columns)\n",
    "\n",
    "# Create product/business issue tag and drop those who have both tagged to simplify our analysis\n",
    "df_tag = df[tag_columns].fillna(0).applymap(lambda x: 1 if x>0 else 0)\n",
    "df_tag[\"product_issue\"] = df_tag[product_issues_columns].max(axis=1)\n",
    "df_tag[\"business_issue\"] = df_tag[business_issues_columns].max(axis=1)\n",
    "\n",
    "# Added to remove zeroed vectors seqs\n",
    "df_tag = df_tag.loc[feat_seq.sum(axis=2).sum(axis=1)!=0]\n",
    "feat_seq = feat_seq[feat_seq.sum(axis=2).sum(axis=1)!=0]\n",
    "\n",
    "feat_seq = feat_seq[(df_tag[[\"product_issue\", \"business_issue\"]].sum(axis=1) != 2).values]\n",
    "df_tag = df_tag[df_tag[[\"product_issue\", \"business_issue\"]].sum(axis=1) != 2]\n",
    "\n",
    "feat_seq = feat_seq[(df_tag[[\"product_issue\", \"business_issue\"]].sum(axis=1) != 0).values]\n",
    "df_tag = df_tag[df_tag[[\"product_issue\", \"business_issue\"]].sum(axis=1) != 0]\n",
    "\n",
    "\n",
    "assert df_tag.shape[0] == feat_seq.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = feat_seq\n",
    "Y = df_tag\n",
    "\n",
    "X_test = X[-100::]\n",
    "Y_test = Y.iloc[-100::]\n",
    "\n",
    "X_t = X[0:-100]\n",
    "Y_t = Y.iloc[0:-100]\n",
    "\n",
    "# Sanity check\n",
    "assert X_test.shape[0] + X_t.shape[0] == X.shape[0]\n",
    "assert Y_test.shape[0] + Y_t.shape[0] == Y.shape[0]\n",
    "\n",
    "seed = 2020\n",
    "split = 0.95  # We have a bunch of data\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X_t, Y_t, train_size=split, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_3 (LSTM)                (None, 128)               117248    \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_49 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_40 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "activation_50 (Activation)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_41 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 1)                 33        \n",
      "_________________________________________________________________\n",
      "activation_51 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 127,617\n",
      "Trainable params: 127,617\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Pre-loading\n",
    "filepath=\"weights-improvement-SeqModel-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "embedding_dim = 100\n",
    "batch_size = 128\n",
    "epochs = 40\n",
    "stack = 3\n",
    "\n",
    "\n",
    "def create_RNN(n_dense=6,\n",
    "               dense_units=16,\n",
    "               max_seq_len=25,\n",
    "               lstm_cell=128,\n",
    "               activation='selu',\n",
    "               dropout=layers.Dropout,\n",
    "               dropout_rate=0.1,\n",
    "               kernel_initializer='lecun_normal',\n",
    "               optimizer='adam',\n",
    "               num_classes=1,\n",
    "               max_words=100):\n",
    "    \n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(layers.LSTM(lstm_cell, input_shape=(max_seq_len, max_words)))\n",
    "    \n",
    "    for i in range(n_dense - 1):\n",
    "        model.add(layers.Dense(dense_units//(2+i*2), kernel_initializer=kernel_initializer))\n",
    "        model.add(layers.Activation(activation))\n",
    "        model.add(dropout(dropout_rate))\n",
    "\n",
    "    model.add(layers.Dense(1))\n",
    "    model.add(layers.Activation('sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "network1 = {\n",
    "    'n_dense': stack,\n",
    "    \"lstm_cell\": 64,\n",
    "    'dense_units': 64,\n",
    "    'activation': 'relu',\n",
    "    'dropout': layers.Dropout,\n",
    "    'dropout_rate': 0.5,\n",
    "    \"max_words\": embedding_dim,\n",
    "    'kernel_initializer': 'glorot_uniform',\n",
    "    'optimizer': 'adam'\n",
    "}\n",
    "\n",
    "model_rnn = create_RNN(num_classes=num_classes, **network1)\n",
    "model_rnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 24671 samples, validate on 1299 samples\n",
      "Epoch 1/40\n",
      "24448/24671 [============================>.] - ETA: 0s - loss: 0.4462 - acc: 0.7926\n",
      "Epoch 00001: val_loss improved from -inf to 0.31611, saving model to weights-improvement-SeqModel-01-0.32.hdf5\n",
      "24671/24671 [==============================] - 8s 335us/sample - loss: 0.4451 - acc: 0.7928 - val_loss: 0.3161 - val_acc: 0.8630\n",
      "Epoch 2/40\n",
      "24448/24671 [============================>.] - ETA: 0s - loss: 0.3092 - acc: 0.8662\n",
      "Epoch 00002: val_loss did not improve from 0.31611\n",
      "24671/24671 [==============================] - 7s 286us/sample - loss: 0.3088 - acc: 0.8665 - val_loss: 0.2614 - val_acc: 0.8845\n",
      "Epoch 3/40\n",
      "24448/24671 [============================>.] - ETA: 0s - loss: 0.2654 - acc: 0.8905\n",
      "Epoch 00003: val_loss did not improve from 0.31611\n",
      "24671/24671 [==============================] - 7s 287us/sample - loss: 0.2650 - acc: 0.8908 - val_loss: 0.2415 - val_acc: 0.8984\n",
      "Epoch 4/40\n",
      "24448/24671 [============================>.] - ETA: 0s - loss: 0.2494 - acc: 0.8969\n",
      "Epoch 00004: val_loss did not improve from 0.31611\n",
      "24671/24671 [==============================] - 7s 289us/sample - loss: 0.2494 - acc: 0.8967 - val_loss: 0.2376 - val_acc: 0.8907\n",
      "Epoch 5/40\n",
      "24448/24671 [============================>.] - ETA: 0s - loss: 0.2258 - acc: 0.9090\n",
      "Epoch 00005: val_loss did not improve from 0.31611\n",
      "24671/24671 [==============================] - 7s 291us/sample - loss: 0.2261 - acc: 0.9088 - val_loss: 0.2418 - val_acc: 0.9030\n",
      "Epoch 6/40\n",
      "24448/24671 [============================>.] - ETA: 0s - loss: 0.2072 - acc: 0.9159\n",
      "Epoch 00006: val_loss did not improve from 0.31611\n",
      "24671/24671 [==============================] - 7s 287us/sample - loss: 0.2084 - acc: 0.9158 - val_loss: 0.2411 - val_acc: 0.9061\n",
      "Epoch 7/40\n",
      "24448/24671 [============================>.] - ETA: 0s - loss: 0.1867 - acc: 0.9231\n",
      "Epoch 00007: val_loss did not improve from 0.31611\n",
      "24671/24671 [==============================] - 7s 287us/sample - loss: 0.1868 - acc: 0.9229 - val_loss: 0.2609 - val_acc: 0.8907\n",
      "Epoch 8/40\n",
      "24448/24671 [============================>.] - ETA: 0s - loss: 0.1710 - acc: 0.9302\n",
      "Epoch 00008: val_loss did not improve from 0.31611\n",
      "24671/24671 [==============================] - 7s 284us/sample - loss: 0.1712 - acc: 0.9302 - val_loss: 0.2590 - val_acc: 0.8992\n",
      "Epoch 9/40\n",
      "24448/24671 [============================>.] - ETA: 0s - loss: 0.1531 - acc: 0.9368\n",
      "Epoch 00009: val_loss did not improve from 0.31611\n",
      "24671/24671 [==============================] - 7s 287us/sample - loss: 0.1530 - acc: 0.9368 - val_loss: 0.2810 - val_acc: 0.9045\n",
      "Epoch 10/40\n",
      "24448/24671 [============================>.] - ETA: 0s - loss: 0.1392 - acc: 0.9439\n",
      "Epoch 00010: val_loss improved from 0.31611 to 0.35380, saving model to weights-improvement-SeqModel-10-0.35.hdf5\n",
      "24671/24671 [==============================] - 7s 288us/sample - loss: 0.1391 - acc: 0.9439 - val_loss: 0.3538 - val_acc: 0.9007\n",
      "Epoch 11/40\n",
      "24448/24671 [============================>.] - ETA: 0s - loss: 0.1291 - acc: 0.9478\n",
      "Epoch 00011: val_loss did not improve from 0.35380\n",
      "24671/24671 [==============================] - 7s 287us/sample - loss: 0.1288 - acc: 0.9480 - val_loss: 0.3219 - val_acc: 0.9084\n",
      "Epoch 12/40\n",
      "24448/24671 [============================>.] - ETA: 0s - loss: 0.1107 - acc: 0.9559\n",
      "Epoch 00012: val_loss improved from 0.35380 to 0.36816, saving model to weights-improvement-SeqModel-12-0.37.hdf5\n",
      "24671/24671 [==============================] - 7s 288us/sample - loss: 0.1104 - acc: 0.9560 - val_loss: 0.3682 - val_acc: 0.9061\n",
      "Epoch 13/40\n",
      "24448/24671 [============================>.] - ETA: 0s - loss: 0.0969 - acc: 0.9621\n",
      "Epoch 00013: val_loss improved from 0.36816 to 0.43062, saving model to weights-improvement-SeqModel-13-0.43.hdf5\n",
      "24671/24671 [==============================] - 7s 287us/sample - loss: 0.0967 - acc: 0.9621 - val_loss: 0.4306 - val_acc: 0.8968\n",
      "Epoch 14/40\n",
      "24448/24671 [============================>.] - ETA: 0s - loss: 0.0853 - acc: 0.9658\n",
      "Epoch 00014: val_loss improved from 0.43062 to 0.44453, saving model to weights-improvement-SeqModel-14-0.44.hdf5\n",
      "24671/24671 [==============================] - 7s 290us/sample - loss: 0.0858 - acc: 0.9656 - val_loss: 0.4445 - val_acc: 0.8938\n",
      "Epoch 15/40\n",
      "24448/24671 [============================>.] - ETA: 0s - loss: 0.0820 - acc: 0.9688\n",
      "Epoch 00015: val_loss improved from 0.44453 to 0.46771, saving model to weights-improvement-SeqModel-15-0.47.hdf5\n",
      "24671/24671 [==============================] - 7s 291us/sample - loss: 0.0819 - acc: 0.9687 - val_loss: 0.4677 - val_acc: 0.8930\n",
      "Epoch 16/40\n",
      "24448/24671 [============================>.] - ETA: 0s - loss: 0.0748 - acc: 0.9723\n",
      "Epoch 00016: val_loss improved from 0.46771 to 0.48477, saving model to weights-improvement-SeqModel-16-0.48.hdf5\n",
      "24671/24671 [==============================] - 7s 290us/sample - loss: 0.0749 - acc: 0.9722 - val_loss: 0.4848 - val_acc: 0.8876\n",
      "Epoch 17/40\n",
      "24448/24671 [============================>.] - ETA: 0s - loss: 0.0648 - acc: 0.9766\n",
      "Epoch 00017: val_loss improved from 0.48477 to 0.50061, saving model to weights-improvement-SeqModel-17-0.50.hdf5\n",
      "24671/24671 [==============================] - 7s 288us/sample - loss: 0.0648 - acc: 0.9767 - val_loss: 0.5006 - val_acc: 0.8968\n",
      "Epoch 18/40\n",
      "24448/24671 [============================>.] - ETA: 0s - loss: 0.0601 - acc: 0.9777\n",
      "Epoch 00018: val_loss improved from 0.50061 to 0.51202, saving model to weights-improvement-SeqModel-18-0.51.hdf5\n",
      "24671/24671 [==============================] - 7s 292us/sample - loss: 0.0602 - acc: 0.9777 - val_loss: 0.5120 - val_acc: 0.8899\n",
      "Epoch 19/40\n",
      "24448/24671 [============================>.] - ETA: 0s - loss: 0.0615 - acc: 0.9777\n",
      "Epoch 00019: val_loss did not improve from 0.51202\n",
      "24671/24671 [==============================] - 7s 287us/sample - loss: 0.0618 - acc: 0.9775 - val_loss: 0.5097 - val_acc: 0.8761\n",
      "Epoch 20/40\n",
      "24448/24671 [============================>.] - ETA: 0s - loss: 0.0571 - acc: 0.9789\n",
      "Epoch 00020: val_loss improved from 0.51202 to 0.60775, saving model to weights-improvement-SeqModel-20-0.61.hdf5\n",
      "24671/24671 [==============================] - 7s 286us/sample - loss: 0.0571 - acc: 0.9789 - val_loss: 0.6077 - val_acc: 0.8907\n",
      "Epoch 21/40\n",
      "24448/24671 [============================>.] - ETA: 0s - loss: 0.0503 - acc: 0.9816\n",
      "Epoch 00021: val_loss did not improve from 0.60775\n",
      "24671/24671 [==============================] - 7s 286us/sample - loss: 0.0503 - acc: 0.9816 - val_loss: 0.5584 - val_acc: 0.8884\n",
      "Epoch 22/40\n",
      "24448/24671 [============================>.] - ETA: 0s - loss: 0.0422 - acc: 0.9850\n",
      "Epoch 00022: val_loss improved from 0.60775 to 0.70788, saving model to weights-improvement-SeqModel-22-0.71.hdf5\n",
      "24671/24671 [==============================] - 7s 286us/sample - loss: 0.0421 - acc: 0.9851 - val_loss: 0.7079 - val_acc: 0.8945\n",
      "Epoch 23/40\n",
      "24448/24671 [============================>.] - ETA: 0s - loss: 0.0347 - acc: 0.9886\n",
      "Epoch 00023: val_loss improved from 0.70788 to 0.71604, saving model to weights-improvement-SeqModel-23-0.72.hdf5\n",
      "24671/24671 [==============================] - 7s 289us/sample - loss: 0.0346 - acc: 0.9886 - val_loss: 0.7160 - val_acc: 0.8884\n",
      "Epoch 24/40\n",
      "24448/24671 [============================>.] - ETA: 0s - loss: 0.0350 - acc: 0.9885\n",
      "Epoch 00024: val_loss did not improve from 0.71604\n",
      "24671/24671 [==============================] - 7s 287us/sample - loss: 0.0348 - acc: 0.9886 - val_loss: 0.6888 - val_acc: 0.8930\n",
      "Epoch 25/40\n",
      "24448/24671 [============================>.] - ETA: 0s - loss: 0.0373 - acc: 0.9881\n",
      "Epoch 00025: val_loss did not improve from 0.71604\n",
      "24671/24671 [==============================] - 7s 287us/sample - loss: 0.0373 - acc: 0.9881 - val_loss: 0.7122 - val_acc: 0.8776\n",
      "Epoch 26/40\n",
      "24448/24671 [============================>.] - ETA: 0s - loss: 0.0372 - acc: 0.9873\n",
      "Epoch 00026: val_loss did not improve from 0.71604\n",
      "24671/24671 [==============================] - 7s 290us/sample - loss: 0.0373 - acc: 0.9872 - val_loss: 0.7089 - val_acc: 0.8861\n",
      "Epoch 27/40\n",
      "24448/24671 [============================>.] - ETA: 0s - loss: 0.0325 - acc: 0.9898\n",
      "Epoch 00027: val_loss improved from 0.71604 to 0.71983, saving model to weights-improvement-SeqModel-27-0.72.hdf5\n",
      "24671/24671 [==============================] - 7s 291us/sample - loss: 0.0324 - acc: 0.9898 - val_loss: 0.7198 - val_acc: 0.8884\n",
      "Epoch 28/40\n",
      "24448/24671 [============================>.] - ETA: 0s - loss: 0.0285 - acc: 0.9901\n",
      "Epoch 00028: val_loss improved from 0.71983 to 0.83926, saving model to weights-improvement-SeqModel-28-0.84.hdf5\n",
      "24671/24671 [==============================] - 7s 292us/sample - loss: 0.0284 - acc: 0.9901 - val_loss: 0.8393 - val_acc: 0.8945\n",
      "Epoch 29/40\n",
      "24448/24671 [============================>.] - ETA: 0s - loss: 0.0284 - acc: 0.9908\n",
      "Epoch 00029: val_loss did not improve from 0.83926\n",
      "24671/24671 [==============================] - 7s 287us/sample - loss: 0.0287 - acc: 0.9907 - val_loss: 0.7963 - val_acc: 0.8838\n",
      "Epoch 30/40\n",
      "24448/24671 [============================>.] - ETA: 0s - loss: 0.0242 - acc: 0.9918\n",
      "Epoch 00030: val_loss improved from 0.83926 to 0.91120, saving model to weights-improvement-SeqModel-30-0.91.hdf5\n",
      "24671/24671 [==============================] - 7s 289us/sample - loss: 0.0241 - acc: 0.9919 - val_loss: 0.9112 - val_acc: 0.8884\n",
      "Epoch 31/40\n",
      "24448/24671 [============================>.] - ETA: 0s - loss: 0.0186 - acc: 0.9944\n",
      "Epoch 00031: val_loss improved from 0.91120 to 0.92244, saving model to weights-improvement-SeqModel-31-0.92.hdf5\n",
      "24671/24671 [==============================] - 7s 295us/sample - loss: 0.0187 - acc: 0.9944 - val_loss: 0.9224 - val_acc: 0.8938\n",
      "Epoch 32/40\n",
      "24448/24671 [============================>.] - ETA: 0s - loss: 0.0296 - acc: 0.9906\n",
      "Epoch 00032: val_loss did not improve from 0.92244\n",
      "24671/24671 [==============================] - 7s 287us/sample - loss: 0.0297 - acc: 0.9905 - val_loss: 0.8422 - val_acc: 0.8822\n",
      "Epoch 33/40\n",
      "24448/24671 [============================>.] - ETA: 0s - loss: 0.0231 - acc: 0.9926\n",
      "Epoch 00033: val_loss did not improve from 0.92244\n",
      "24671/24671 [==============================] - 7s 287us/sample - loss: 0.0231 - acc: 0.9926 - val_loss: 0.8921 - val_acc: 0.8891\n",
      "Epoch 34/40\n",
      "24448/24671 [============================>.] - ETA: 0s - loss: 0.0186 - acc: 0.9938\n",
      "Epoch 00034: val_loss improved from 0.92244 to 0.96362, saving model to weights-improvement-SeqModel-34-0.96.hdf5\n",
      "24671/24671 [==============================] - 7s 288us/sample - loss: 0.0188 - acc: 0.9937 - val_loss: 0.9636 - val_acc: 0.8822\n",
      "Epoch 35/40\n",
      "24448/24671 [============================>.] - ETA: 0s - loss: 0.0237 - acc: 0.9930\n",
      "Epoch 00035: val_loss did not improve from 0.96362\n",
      "24671/24671 [==============================] - 7s 285us/sample - loss: 0.0236 - acc: 0.9930 - val_loss: 0.9293 - val_acc: 0.8822\n",
      "Epoch 36/40\n",
      "24448/24671 [============================>.] - ETA: 0s - loss: 0.0245 - acc: 0.9918\n",
      "Epoch 00036: val_loss did not improve from 0.96362\n",
      "24671/24671 [==============================] - 7s 286us/sample - loss: 0.0245 - acc: 0.9918 - val_loss: 0.9149 - val_acc: 0.8868\n",
      "Epoch 37/40\n",
      "24448/24671 [============================>.] - ETA: 0s - loss: 0.0191 - acc: 0.9933\n",
      "Epoch 00037: val_loss improved from 0.96362 to 1.03690, saving model to weights-improvement-SeqModel-37-1.04.hdf5\n",
      "24671/24671 [==============================] - 7s 291us/sample - loss: 0.0195 - acc: 0.9931 - val_loss: 1.0369 - val_acc: 0.8799\n",
      "Epoch 38/40\n",
      "24448/24671 [============================>.] - ETA: 0s - loss: 0.0224 - acc: 0.9929\n",
      "Epoch 00038: val_loss did not improve from 1.03690\n",
      "24671/24671 [==============================] - 7s 289us/sample - loss: 0.0223 - acc: 0.9929 - val_loss: 0.9127 - val_acc: 0.8899\n",
      "Epoch 39/40\n",
      "24448/24671 [============================>.] - ETA: 0s - loss: 0.0240 - acc: 0.9927\n",
      "Epoch 00039: val_loss did not improve from 1.03690\n",
      "24671/24671 [==============================] - 7s 284us/sample - loss: 0.0241 - acc: 0.9927 - val_loss: 0.8746 - val_acc: 0.8791\n",
      "Epoch 40/40\n",
      "24448/24671 [============================>.] - ETA: 0s - loss: 0.0324 - acc: 0.9899\n",
      "Epoch 00040: val_loss did not improve from 1.03690\n",
      "24671/24671 [==============================] - 7s 286us/sample - loss: 0.0326 - acc: 0.9899 - val_loss: 0.8225 - val_acc: 0.8807\n"
     ]
    }
   ],
   "source": [
    "history_model1 = model_rnn.fit(X_train,\n",
    "                            y_train[\"product_issue\"].values.reshape(-1,1),\n",
    "                            batch_size=batch_size,\n",
    "                            epochs=epochs,\n",
    "                            verbose=1,\n",
    "                            callbacks=callbacks_list,\n",
    "                            validation_data=(X_validation, y_validation[\"product_issue\"].values.reshape(-1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_2 (Bidirection (None, 128)               84480     \n",
      "_________________________________________________________________\n",
      "dense_61 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "activation_61 (Activation)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_48 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_62 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "activation_62 (Activation)   (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dropout_49 (Dropout)         (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_63 (Dense)             (None, 1)                 17        \n",
      "_________________________________________________________________\n",
      "activation_63 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 89,153\n",
      "Trainable params: 89,153\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "def create_BiRNN(n_dense=6,\n",
    "                   dense_units=16,\n",
    "                   max_seq_len=25,\n",
    "                   lstm_cell=128,\n",
    "                   activation='selu',\n",
    "                   dropout=layers.Dropout,\n",
    "                   dropout_rate=0.1,\n",
    "                   bi_merge=\"concat\",\n",
    "                   kernel_initializer='lecun_normal',\n",
    "                   optimizer='adam',\n",
    "                   num_classes=1,\n",
    "                   max_words=100):\n",
    "    \n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(layers.Bidirectional(layers.LSTM(lstm_cell), input_shape=(max_seq_len, max_words) , merge_mode=bi_merge))\n",
    "    \n",
    "    for i in range(n_dense - 1):\n",
    "        model.add(layers.Dense(dense_units//(2+i*2), kernel_initializer=kernel_initializer))\n",
    "        model.add(layers.Activation(activation))\n",
    "        model.add(dropout(dropout_rate))\n",
    "\n",
    "    model.add(layers.Dense(1))\n",
    "    model.add(layers.Activation('sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "network3 = {\n",
    "    'n_dense': stack,\n",
    "    \"lstm_cell\": 64,\n",
    "    'dense_units': 64,\n",
    "    'activation': 'relu',\n",
    "    \"bi_merge\": \"concat\",\n",
    "    'dropout': layers.Dropout,\n",
    "    'dropout_rate': 0.5,\n",
    "    \"max_words\": embedding_dim,\n",
    "    'kernel_initializer': 'glorot_uniform',\n",
    "    'optimizer': 'adam'\n",
    "}\n",
    "\n",
    "model_rnn2 = create_BiRNN(num_classes=num_classes, **network3)\n",
    "model_rnn2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 24671 samples, validate on 1299 samples\n",
      "Epoch 1/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.4540 - acc: 0.7852\n",
      "Epoch 00001: val_acc improved from -inf to 0.88068, saving model to weights-improvement-SeqModelBi-01-0.88.hdf5\n",
      "24671/24671 [==============================] - 13s 534us/sample - loss: 0.4537 - acc: 0.7853 - val_loss: 0.2767 - val_acc: 0.8807\n",
      "Epoch 2/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.3094 - acc: 0.8769\n",
      "Epoch 00002: val_acc improved from 0.88068 to 0.88761, saving model to weights-improvement-SeqModelBi-02-0.89.hdf5\n",
      "24671/24671 [==============================] - 11s 452us/sample - loss: 0.3089 - acc: 0.8770 - val_loss: 0.2520 - val_acc: 0.8876\n",
      "Epoch 3/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.2698 - acc: 0.8906\n",
      "Epoch 00003: val_acc improved from 0.88761 to 0.89222, saving model to weights-improvement-SeqModelBi-03-0.89.hdf5\n",
      "24671/24671 [==============================] - 11s 454us/sample - loss: 0.2696 - acc: 0.8906 - val_loss: 0.2396 - val_acc: 0.8922\n",
      "Epoch 4/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.2418 - acc: 0.9012\n",
      "Epoch 00004: val_acc improved from 0.89222 to 0.89530, saving model to weights-improvement-SeqModelBi-04-0.90.hdf5\n",
      "24671/24671 [==============================] - 12s 466us/sample - loss: 0.2421 - acc: 0.9012 - val_loss: 0.2438 - val_acc: 0.8953\n",
      "Epoch 5/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.2161 - acc: 0.9104\n",
      "Epoch 00005: val_acc did not improve from 0.89530\n",
      "24671/24671 [==============================] - 11s 457us/sample - loss: 0.2167 - acc: 0.9102 - val_loss: 0.2487 - val_acc: 0.8938\n",
      "Epoch 6/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.1955 - acc: 0.9188\n",
      "Epoch 00006: val_acc did not improve from 0.89530\n",
      "24671/24671 [==============================] - 11s 459us/sample - loss: 0.1956 - acc: 0.9187 - val_loss: 0.2564 - val_acc: 0.8915\n",
      "Epoch 7/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.1762 - acc: 0.9288\n",
      "Epoch 00007: val_acc did not improve from 0.89530\n",
      "24671/24671 [==============================] - 11s 453us/sample - loss: 0.1764 - acc: 0.9287 - val_loss: 0.2773 - val_acc: 0.8915\n",
      "Epoch 8/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.1585 - acc: 0.9343\n",
      "Epoch 00008: val_acc improved from 0.89530 to 0.89915, saving model to weights-improvement-SeqModelBi-08-0.90.hdf5\n",
      "24671/24671 [==============================] - 11s 461us/sample - loss: 0.1585 - acc: 0.9343 - val_loss: 0.2817 - val_acc: 0.8992\n",
      "Epoch 9/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.1411 - acc: 0.9402\n",
      "Epoch 00009: val_acc did not improve from 0.89915\n",
      "24671/24671 [==============================] - 11s 454us/sample - loss: 0.1410 - acc: 0.9402 - val_loss: 0.3073 - val_acc: 0.8961\n",
      "Epoch 10/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.1295 - acc: 0.9478\n",
      "Epoch 00010: val_acc improved from 0.89915 to 0.90454, saving model to weights-improvement-SeqModelBi-10-0.90.hdf5\n",
      "24671/24671 [==============================] - 11s 456us/sample - loss: 0.1297 - acc: 0.9477 - val_loss: 0.3399 - val_acc: 0.9045\n",
      "Epoch 11/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.1146 - acc: 0.9530\n",
      "Epoch 00011: val_acc did not improve from 0.90454\n",
      "24671/24671 [==============================] - 11s 452us/sample - loss: 0.1146 - acc: 0.9530 - val_loss: 0.3734 - val_acc: 0.8953\n",
      "Epoch 12/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.1034 - acc: 0.9569\n",
      "Epoch 00012: val_acc did not improve from 0.90454\n",
      "24671/24671 [==============================] - 11s 454us/sample - loss: 0.1036 - acc: 0.9570 - val_loss: 0.4215 - val_acc: 0.8853\n",
      "Epoch 13/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.0990 - acc: 0.9593\n",
      "Epoch 00013: val_acc did not improve from 0.90454\n",
      "24671/24671 [==============================] - 11s 452us/sample - loss: 0.0992 - acc: 0.9593 - val_loss: 0.4410 - val_acc: 0.8930\n",
      "Epoch 14/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.0936 - acc: 0.9636\n",
      "Epoch 00014: val_acc did not improve from 0.90454\n",
      "24671/24671 [==============================] - 11s 455us/sample - loss: 0.0938 - acc: 0.9634 - val_loss: 0.4793 - val_acc: 0.8907\n",
      "Epoch 15/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.0815 - acc: 0.9672\n",
      "Epoch 00015: val_acc did not improve from 0.90454\n",
      "24671/24671 [==============================] - 11s 452us/sample - loss: 0.0816 - acc: 0.9671 - val_loss: 0.5452 - val_acc: 0.8899\n",
      "Epoch 16/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.0774 - acc: 0.9694\n",
      "Epoch 00016: val_acc did not improve from 0.90454\n",
      "24671/24671 [==============================] - 11s 450us/sample - loss: 0.0774 - acc: 0.9694 - val_loss: 0.6322 - val_acc: 0.8876\n",
      "Epoch 17/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.0713 - acc: 0.9716\n",
      "Epoch 00017: val_acc did not improve from 0.90454\n",
      "24671/24671 [==============================] - 11s 452us/sample - loss: 0.0711 - acc: 0.9716 - val_loss: 0.6176 - val_acc: 0.8945\n",
      "Epoch 18/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.0634 - acc: 0.9754\n",
      "Epoch 00018: val_acc did not improve from 0.90454\n",
      "24671/24671 [==============================] - 11s 458us/sample - loss: 0.0634 - acc: 0.9754 - val_loss: 0.6209 - val_acc: 0.8884\n",
      "Epoch 19/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.0545 - acc: 0.9797\n",
      "Epoch 00019: val_acc did not improve from 0.90454\n",
      "24671/24671 [==============================] - 11s 454us/sample - loss: 0.0544 - acc: 0.9798 - val_loss: 0.7086 - val_acc: 0.8876\n",
      "Epoch 20/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.0551 - acc: 0.9798\n",
      "Epoch 00020: val_acc did not improve from 0.90454\n",
      "24671/24671 [==============================] - 11s 450us/sample - loss: 0.0551 - acc: 0.9798 - val_loss: 0.7756 - val_acc: 0.8876\n",
      "Epoch 21/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.0562 - acc: 0.9791\n",
      "Epoch 00021: val_acc did not improve from 0.90454\n",
      "24671/24671 [==============================] - 11s 456us/sample - loss: 0.0561 - acc: 0.9792 - val_loss: 0.7815 - val_acc: 0.8814\n",
      "Epoch 22/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.0453 - acc: 0.9840\n",
      "Epoch 00022: val_acc did not improve from 0.90454\n",
      "24671/24671 [==============================] - 11s 455us/sample - loss: 0.0452 - acc: 0.9840 - val_loss: 0.8084 - val_acc: 0.8891\n",
      "Epoch 23/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.0389 - acc: 0.9849\n",
      "Epoch 00023: val_acc did not improve from 0.90454\n",
      "24671/24671 [==============================] - 11s 456us/sample - loss: 0.0392 - acc: 0.9848 - val_loss: 0.8363 - val_acc: 0.8976\n",
      "Epoch 24/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.0438 - acc: 0.9860\n",
      "Epoch 00024: val_acc did not improve from 0.90454\n",
      "24671/24671 [==============================] - 11s 455us/sample - loss: 0.0439 - acc: 0.9859 - val_loss: 0.8428 - val_acc: 0.8891\n",
      "Epoch 25/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.0455 - acc: 0.9839\n",
      "Epoch 00025: val_acc did not improve from 0.90454\n",
      "24671/24671 [==============================] - 11s 456us/sample - loss: 0.0455 - acc: 0.9839 - val_loss: 0.8249 - val_acc: 0.8730\n",
      "Epoch 26/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.0462 - acc: 0.9839\n",
      "Epoch 00026: val_acc did not improve from 0.90454\n",
      "24671/24671 [==============================] - 11s 455us/sample - loss: 0.0462 - acc: 0.9839 - val_loss: 0.7957 - val_acc: 0.8922\n",
      "Epoch 27/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.0440 - acc: 0.9845\n",
      "Epoch 00027: val_acc did not improve from 0.90454\n",
      "24671/24671 [==============================] - 11s 452us/sample - loss: 0.0440 - acc: 0.9845 - val_loss: 0.8616 - val_acc: 0.8737\n",
      "Epoch 28/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.0349 - acc: 0.9885\n",
      "Epoch 00028: val_acc did not improve from 0.90454\n",
      "24671/24671 [==============================] - 11s 454us/sample - loss: 0.0350 - acc: 0.9884 - val_loss: 0.9435 - val_acc: 0.8845\n",
      "Epoch 29/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.0282 - acc: 0.9901\n",
      "Epoch 00029: val_acc did not improve from 0.90454\n",
      "24671/24671 [==============================] - 11s 456us/sample - loss: 0.0282 - acc: 0.9900 - val_loss: 0.9313 - val_acc: 0.8868\n",
      "Epoch 30/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.0315 - acc: 0.9893\n",
      "Epoch 00030: val_acc did not improve from 0.90454\n",
      "24671/24671 [==============================] - 11s 457us/sample - loss: 0.0315 - acc: 0.9893 - val_loss: 1.0640 - val_acc: 0.8861\n",
      "Epoch 31/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.0367 - acc: 0.9882\n",
      "Epoch 00031: val_acc did not improve from 0.90454\n",
      "24671/24671 [==============================] - 11s 458us/sample - loss: 0.0369 - acc: 0.9881 - val_loss: 0.9389 - val_acc: 0.8891\n",
      "Epoch 32/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.0272 - acc: 0.9910\n",
      "Epoch 00032: val_acc did not improve from 0.90454\n",
      "24671/24671 [==============================] - 11s 457us/sample - loss: 0.0272 - acc: 0.9910 - val_loss: 0.9768 - val_acc: 0.8884\n",
      "Epoch 33/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9905\n",
      "Epoch 00033: val_acc did not improve from 0.90454\n",
      "24671/24671 [==============================] - 11s 457us/sample - loss: 0.0288 - acc: 0.9904 - val_loss: 0.9692 - val_acc: 0.8861\n",
      "Epoch 34/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.0322 - acc: 0.9896\n",
      "Epoch 00034: val_acc did not improve from 0.90454\n",
      "24671/24671 [==============================] - 11s 453us/sample - loss: 0.0321 - acc: 0.9897 - val_loss: 0.9611 - val_acc: 0.8868\n",
      "Epoch 35/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.0265 - acc: 0.9919\n",
      "Epoch 00035: val_acc did not improve from 0.90454\n",
      "24671/24671 [==============================] - 11s 455us/sample - loss: 0.0264 - acc: 0.9919 - val_loss: 1.1011 - val_acc: 0.8807\n",
      "Epoch 36/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.0255 - acc: 0.9916\n",
      "Epoch 00036: val_acc did not improve from 0.90454\n",
      "24671/24671 [==============================] - 11s 456us/sample - loss: 0.0254 - acc: 0.9917 - val_loss: 1.0585 - val_acc: 0.8884\n",
      "Epoch 37/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.0236 - acc: 0.9917\n",
      "Epoch 00037: val_acc did not improve from 0.90454\n",
      "24671/24671 [==============================] - 11s 457us/sample - loss: 0.0235 - acc: 0.9918 - val_loss: 1.0951 - val_acc: 0.8745\n",
      "Epoch 38/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.0235 - acc: 0.9922\n",
      "Epoch 00038: val_acc did not improve from 0.90454\n",
      "24671/24671 [==============================] - 11s 458us/sample - loss: 0.0236 - acc: 0.9921 - val_loss: 1.1368 - val_acc: 0.8784\n",
      "Epoch 39/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.0243 - acc: 0.9923\n",
      "Epoch 00039: val_acc did not improve from 0.90454\n",
      "24671/24671 [==============================] - 11s 456us/sample - loss: 0.0246 - acc: 0.9922 - val_loss: 1.1225 - val_acc: 0.8761\n",
      "Epoch 40/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.0320 - acc: 0.9898\n",
      "Epoch 00040: val_acc did not improve from 0.90454\n",
      "24671/24671 [==============================] - 11s 455us/sample - loss: 0.0320 - acc: 0.9898 - val_loss: 1.1180 - val_acc: 0.8799\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Pre-loading\n",
    "filepath=\"weights-improvement-SeqModelBi-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "history_model_rnn_2 = model_rnn2.fit(X_train,\n",
    "                            y_train[\"product_issue\"].values.reshape(-1,1),\n",
    "                            batch_size=batch_size,\n",
    "                            epochs=epochs,\n",
    "                            verbose=1,\n",
    "                            callbacks=callbacks_list,\n",
    "                            validation_data=(X_validation, y_validation[\"product_issue\"].values.reshape(-1,1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remarks\n",
    "\n",
    "Using recurrent networks did not improve our model accuracy. Lets double the sequence size and check how it performs with more spatial information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recomputing data preprocessing with seq=50...\n",
    "\n",
    "from src.text_formatting import get_tokens_from_RA_df\n",
    "\n",
    "def get_review_embbedings_seq(review_seq, model: \"gensim.model\", seq_max=25):\n",
    "    final_seq = np.zeros([seq_max,model.vector_size])\n",
    "    for i,w in enumerate(review_seq):\n",
    "        if i<seq_max:\n",
    "            try:\n",
    "                final_seq[i,:] = model.wv[w]\n",
    "            except Exception as err:\n",
    "                print(err)\n",
    "    return final_seq\n",
    "\n",
    "max_seq = 50\n",
    "toks = get_tokens_from_RA_df(df)\n",
    "feat_seq = np.array([get_review_embbedings_seq(token, model, seq_max=max_seq) for token in toks])\n",
    "\n",
    "# Sanity check\n",
    "assert feat_seq.shape == (len(toks), max_seq, model.vector_size)\n",
    "\n",
    "# Pre-load tag columns\n",
    "product_issues_columns = ['product_issues_Quality','product_issues_Damaged', 'product_issues_Electrical problems','product_issues_Missing pieces']\n",
    "business_issues_columns = ['business_issues_Payment','business_issues_Maintenance', 'business_issues_Customer Services','business_issues_Delivery', 'business_issues_Online Services']\n",
    "tag_columns = product_issues_columns.copy()\n",
    "tag_columns.extend(business_issues_columns)\n",
    "\n",
    "# Create product/business issue tag and drop those who have both tagged to simplify our analysis\n",
    "df_tag = df[tag_columns].fillna(0).applymap(lambda x: 1 if x>0 else 0)\n",
    "df_tag[\"product_issue\"] = df_tag[product_issues_columns].max(axis=1)\n",
    "df_tag[\"business_issue\"] = df_tag[business_issues_columns].max(axis=1)\n",
    "\n",
    "# Added to remove zeroed vectors seqs\n",
    "df_tag = df_tag.loc[feat_seq.sum(axis=2).sum(axis=1)!=0]\n",
    "feat_seq = feat_seq[feat_seq.sum(axis=2).sum(axis=1)!=0]\n",
    "\n",
    "feat_seq = feat_seq[(df_tag[[\"product_issue\", \"business_issue\"]].sum(axis=1) != 2).values]\n",
    "df_tag = df_tag[df_tag[[\"product_issue\", \"business_issue\"]].sum(axis=1) != 2]\n",
    "\n",
    "feat_seq = feat_seq[(df_tag[[\"product_issue\", \"business_issue\"]].sum(axis=1) != 0).values]\n",
    "df_tag = df_tag[df_tag[[\"product_issue\", \"business_issue\"]].sum(axis=1) != 0]\n",
    "\n",
    "\n",
    "assert df_tag.shape[0] == feat_seq.shape[0]\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = feat_seq\n",
    "Y = df_tag\n",
    "\n",
    "X_test = X[-100::]\n",
    "Y_test = Y.iloc[-100::]\n",
    "\n",
    "X_t = X[0:-100]\n",
    "Y_t = Y.iloc[0:-100]\n",
    "\n",
    "# Sanity check\n",
    "assert X_test.shape[0] + X_t.shape[0] == X.shape[0]\n",
    "assert Y_test.shape[0] + Y_t.shape[0] == Y.shape[0]\n",
    "\n",
    "seed = 2020\n",
    "split = 0.95  # We have a bunch of data\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X_t, Y_t, train_size=split, random_state=seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_12 (LSTM)               (None, 64)                42240     \n",
      "_________________________________________________________________\n",
      "dense_76 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "activation_76 (Activation)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_58 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_77 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "activation_77 (Activation)   (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dropout_59 (Dropout)         (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_78 (Dense)             (None, 1)                 17        \n",
      "_________________________________________________________________\n",
      "activation_78 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 44,865\n",
      "Trainable params: 44,865\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Pre-loading\n",
    "filepath=\"weights-improvement-SeqModel50-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "embedding_dim = 100\n",
    "batch_size = 128\n",
    "epochs = 40\n",
    "stack = 3\n",
    "\n",
    "\n",
    "def create_RNN(n_dense=6,\n",
    "               dense_units=16,\n",
    "               max_seq_len=25,\n",
    "               lstm_cell=128,\n",
    "               activation='selu',\n",
    "               dropout=layers.Dropout,\n",
    "               dropout_rate=0.1,\n",
    "               kernel_initializer='lecun_normal',\n",
    "               optimizer='adam',\n",
    "               num_classes=1,\n",
    "               max_words=100):\n",
    "    \n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(layers.LSTM(lstm_cell, input_shape=(max_seq_len, max_words)))\n",
    "    \n",
    "    for i in range(n_dense - 1):\n",
    "        model.add(layers.Dense(dense_units//(2+i*2), kernel_initializer=kernel_initializer))\n",
    "        model.add(layers.Activation(activation))\n",
    "        model.add(dropout(dropout_rate))\n",
    "\n",
    "    model.add(layers.Dense(1))\n",
    "    model.add(layers.Activation('sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "network1 = {\n",
    "    'n_dense': stack,\n",
    "    \"lstm_cell\": 64,\n",
    "    'dense_units': 64,\n",
    "    \"max_seq_len\":max_seq,\n",
    "    'activation': 'relu',\n",
    "    'dropout': layers.Dropout,\n",
    "    'dropout_rate': 0.5,\n",
    "    \"max_words\": embedding_dim,\n",
    "    'kernel_initializer': 'glorot_uniform',\n",
    "    'optimizer': 'adam'\n",
    "}\n",
    "\n",
    "model_rnn = create_RNN(num_classes=num_classes, **network1)\n",
    "model_rnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 24671 samples, validate on 1299 samples\n",
      "Epoch 1/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.5472 - acc: 0.7664\n",
      "Epoch 00001: val_acc improved from -inf to 0.79061, saving model to weights-improvement-SeqModel50-01-0.79.hdf5\n",
      "24671/24671 [==============================] - 14s 585us/sample - loss: 0.5472 - acc: 0.7664 - val_loss: 0.4620 - val_acc: 0.7906\n",
      "Epoch 2/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.4616 - acc: 0.7898\n",
      "Epoch 00002: val_acc improved from 0.79061 to 0.80600, saving model to weights-improvement-SeqModel50-02-0.81.hdf5\n",
      "24671/24671 [==============================] - 12s 497us/sample - loss: 0.4615 - acc: 0.7899 - val_loss: 0.3882 - val_acc: 0.8060\n",
      "Epoch 3/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.3812 - acc: 0.8096\n",
      "Epoch 00003: val_acc improved from 0.80600 to 0.84604, saving model to weights-improvement-SeqModel50-03-0.85.hdf5\n",
      "24671/24671 [==============================] - 12s 502us/sample - loss: 0.3810 - acc: 0.8098 - val_loss: 0.3341 - val_acc: 0.8460\n",
      "Epoch 4/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.3321 - acc: 0.8416\n",
      "Epoch 00004: val_acc improved from 0.84604 to 0.84758, saving model to weights-improvement-SeqModel50-04-0.85.hdf5\n",
      "24671/24671 [==============================] - 12s 499us/sample - loss: 0.3323 - acc: 0.8415 - val_loss: 0.3095 - val_acc: 0.8476\n",
      "Epoch 5/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.3133 - acc: 0.8476\n",
      "Epoch 00005: val_acc improved from 0.84758 to 0.85912, saving model to weights-improvement-SeqModel50-05-0.86.hdf5\n",
      "24671/24671 [==============================] - 12s 498us/sample - loss: 0.3133 - acc: 0.8477 - val_loss: 0.2948 - val_acc: 0.8591\n",
      "Epoch 6/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.3027 - acc: 0.8564\n",
      "Epoch 00006: val_acc improved from 0.85912 to 0.86836, saving model to weights-improvement-SeqModel50-06-0.87.hdf5\n",
      "24671/24671 [==============================] - 12s 492us/sample - loss: 0.3031 - acc: 0.8561 - val_loss: 0.2893 - val_acc: 0.8684\n",
      "Epoch 7/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.2901 - acc: 0.8705\n",
      "Epoch 00007: val_acc did not improve from 0.86836\n",
      "24671/24671 [==============================] - 12s 488us/sample - loss: 0.2900 - acc: 0.8706 - val_loss: 0.2877 - val_acc: 0.8676\n",
      "Epoch 8/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.2709 - acc: 0.8799\n",
      "Epoch 00008: val_acc improved from 0.86836 to 0.87991, saving model to weights-improvement-SeqModel50-08-0.88.hdf5\n",
      "24671/24671 [==============================] - 12s 484us/sample - loss: 0.2710 - acc: 0.8799 - val_loss: 0.2774 - val_acc: 0.8799\n",
      "Epoch 9/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.2613 - acc: 0.8863\n",
      "Epoch 00009: val_acc did not improve from 0.87991\n",
      "24671/24671 [==============================] - 12s 481us/sample - loss: 0.2610 - acc: 0.8865 - val_loss: 0.2814 - val_acc: 0.8745\n",
      "Epoch 10/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.2565 - acc: 0.8949\n",
      "Epoch 00010: val_acc did not improve from 0.87991\n",
      "24671/24671 [==============================] - 12s 498us/sample - loss: 0.2565 - acc: 0.8947 - val_loss: 0.2692 - val_acc: 0.8776\n",
      "Epoch 11/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.2549 - acc: 0.8970\n",
      "Epoch 00011: val_acc improved from 0.87991 to 0.88607, saving model to weights-improvement-SeqModel50-11-0.89.hdf5\n",
      "24671/24671 [==============================] - 13s 508us/sample - loss: 0.2549 - acc: 0.8970 - val_loss: 0.2724 - val_acc: 0.8861\n",
      "Epoch 12/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.2484 - acc: 0.8975\n",
      "Epoch 00012: val_acc did not improve from 0.88607\n",
      "24671/24671 [==============================] - 12s 502us/sample - loss: 0.2488 - acc: 0.8974 - val_loss: 0.2840 - val_acc: 0.8799\n",
      "Epoch 13/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.2336 - acc: 0.9039\n",
      "Epoch 00013: val_acc did not improve from 0.88607\n",
      "24671/24671 [==============================] - 12s 501us/sample - loss: 0.2339 - acc: 0.9039 - val_loss: 0.2875 - val_acc: 0.8791\n",
      "Epoch 14/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.2261 - acc: 0.9069\n",
      "Epoch 00014: val_acc did not improve from 0.88607\n",
      "24671/24671 [==============================] - 12s 500us/sample - loss: 0.2262 - acc: 0.9069 - val_loss: 0.2792 - val_acc: 0.8776\n",
      "Epoch 15/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.2107 - acc: 0.9116\n",
      "Epoch 00015: val_acc did not improve from 0.88607\n",
      "24671/24671 [==============================] - 12s 504us/sample - loss: 0.2106 - acc: 0.9117 - val_loss: 0.2796 - val_acc: 0.8845\n",
      "Epoch 16/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.2025 - acc: 0.9169\n",
      "Epoch 00016: val_acc improved from 0.88607 to 0.88915, saving model to weights-improvement-SeqModel50-16-0.89.hdf5\n",
      "24671/24671 [==============================] - 12s 505us/sample - loss: 0.2024 - acc: 0.9168 - val_loss: 0.2761 - val_acc: 0.8891\n",
      "Epoch 17/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.1972 - acc: 0.9201\n",
      "Epoch 00017: val_acc did not improve from 0.88915\n",
      "24671/24671 [==============================] - 12s 498us/sample - loss: 0.1971 - acc: 0.9202 - val_loss: 0.2819 - val_acc: 0.8861\n",
      "Epoch 18/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.2036 - acc: 0.9165\n",
      "Epoch 00018: val_acc did not improve from 0.88915\n",
      "24671/24671 [==============================] - 12s 498us/sample - loss: 0.2037 - acc: 0.9164 - val_loss: 0.2993 - val_acc: 0.8807\n",
      "Epoch 19/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.1929 - acc: 0.9211\n",
      "Epoch 00019: val_acc did not improve from 0.88915\n",
      "24671/24671 [==============================] - 12s 501us/sample - loss: 0.1926 - acc: 0.9212 - val_loss: 0.3100 - val_acc: 0.8799\n",
      "Epoch 20/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.1785 - acc: 0.9253\n",
      "Epoch 00020: val_acc did not improve from 0.88915\n",
      "24671/24671 [==============================] - 12s 501us/sample - loss: 0.1785 - acc: 0.9253 - val_loss: 0.3267 - val_acc: 0.8884\n",
      "Epoch 21/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.1727 - acc: 0.9274\n",
      "Epoch 00021: val_acc did not improve from 0.88915\n",
      "24671/24671 [==============================] - 12s 503us/sample - loss: 0.1728 - acc: 0.9274 - val_loss: 0.3462 - val_acc: 0.8784\n",
      "Epoch 22/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.1838 - acc: 0.9219\n",
      "Epoch 00022: val_acc did not improve from 0.88915\n",
      "24671/24671 [==============================] - 12s 499us/sample - loss: 0.1839 - acc: 0.9218 - val_loss: 0.3447 - val_acc: 0.8768\n",
      "Epoch 23/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.1767 - acc: 0.9279\n",
      "Epoch 00023: val_acc did not improve from 0.88915\n",
      "24671/24671 [==============================] - 12s 498us/sample - loss: 0.1768 - acc: 0.9276 - val_loss: 0.3356 - val_acc: 0.8768\n",
      "Epoch 24/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.1653 - acc: 0.9319\n",
      "Epoch 00024: val_acc did not improve from 0.88915\n",
      "24671/24671 [==============================] - 12s 490us/sample - loss: 0.1655 - acc: 0.9319 - val_loss: 0.3278 - val_acc: 0.8753\n",
      "Epoch 25/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.1603 - acc: 0.9344\n",
      "Epoch 00025: val_acc did not improve from 0.88915\n",
      "24671/24671 [==============================] - 12s 498us/sample - loss: 0.1602 - acc: 0.9343 - val_loss: 0.3611 - val_acc: 0.8822\n",
      "Epoch 26/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.1729 - acc: 0.9274\n",
      "Epoch 00026: val_acc did not improve from 0.88915\n",
      "24671/24671 [==============================] - 12s 504us/sample - loss: 0.1728 - acc: 0.9275 - val_loss: 0.3490 - val_acc: 0.8684\n",
      "Epoch 27/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.1606 - acc: 0.9322\n",
      "Epoch 00027: val_acc did not improve from 0.88915\n",
      "24671/24671 [==============================] - 12s 503us/sample - loss: 0.1605 - acc: 0.9323 - val_loss: 0.3286 - val_acc: 0.8737\n",
      "Epoch 28/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.1494 - acc: 0.9386\n",
      "Epoch 00028: val_acc did not improve from 0.88915\n",
      "24671/24671 [==============================] - 12s 497us/sample - loss: 0.1498 - acc: 0.9385 - val_loss: 0.3495 - val_acc: 0.8884\n",
      "Epoch 29/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.1412 - acc: 0.9432\n",
      "Epoch 00029: val_acc did not improve from 0.88915\n",
      "24671/24671 [==============================] - 12s 502us/sample - loss: 0.1414 - acc: 0.9431 - val_loss: 0.4225 - val_acc: 0.8737\n",
      "Epoch 30/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.1358 - acc: 0.9438\n",
      "Epoch 00030: val_acc did not improve from 0.88915\n",
      "24671/24671 [==============================] - 12s 502us/sample - loss: 0.1355 - acc: 0.9439 - val_loss: 0.4111 - val_acc: 0.8799\n",
      "Epoch 31/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.1269 - acc: 0.9494\n",
      "Epoch 00031: val_acc did not improve from 0.88915\n",
      "24671/24671 [==============================] - 12s 500us/sample - loss: 0.1269 - acc: 0.9495 - val_loss: 0.5133 - val_acc: 0.8814\n",
      "Epoch 32/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.1226 - acc: 0.9514\n",
      "Epoch 00032: val_acc did not improve from 0.88915\n",
      "24671/24671 [==============================] - 12s 503us/sample - loss: 0.1227 - acc: 0.9513 - val_loss: 0.5263 - val_acc: 0.8768\n",
      "Epoch 33/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.1332 - acc: 0.9460\n",
      "Epoch 00033: val_acc did not improve from 0.88915\n",
      "24671/24671 [==============================] - 12s 501us/sample - loss: 0.1331 - acc: 0.9460 - val_loss: 0.4462 - val_acc: 0.8745\n",
      "Epoch 34/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.1284 - acc: 0.9486\n",
      "Epoch 00034: val_acc did not improve from 0.88915\n",
      "24671/24671 [==============================] - 12s 505us/sample - loss: 0.1286 - acc: 0.9484 - val_loss: 0.4687 - val_acc: 0.8807\n",
      "Epoch 35/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.1108 - acc: 0.9555\n",
      "Epoch 00035: val_acc did not improve from 0.88915\n",
      "24671/24671 [==============================] - 12s 490us/sample - loss: 0.1107 - acc: 0.9555 - val_loss: 0.5988 - val_acc: 0.8814\n",
      "Epoch 36/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.1167 - acc: 0.9529\n",
      "Epoch 00036: val_acc did not improve from 0.88915\n",
      "24671/24671 [==============================] - 12s 494us/sample - loss: 0.1167 - acc: 0.9530 - val_loss: 0.5004 - val_acc: 0.8784\n",
      "Epoch 37/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.1237 - acc: 0.9519\n",
      "Epoch 00037: val_acc did not improve from 0.88915\n",
      "24671/24671 [==============================] - 12s 489us/sample - loss: 0.1239 - acc: 0.9517 - val_loss: 0.4741 - val_acc: 0.8661\n",
      "Epoch 38/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.1131 - acc: 0.9556\n",
      "Epoch 00038: val_acc did not improve from 0.88915\n",
      "24671/24671 [==============================] - 12s 491us/sample - loss: 0.1133 - acc: 0.9555 - val_loss: 0.5490 - val_acc: 0.8737\n",
      "Epoch 39/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.1080 - acc: 0.9587\n",
      "Epoch 00039: val_acc did not improve from 0.88915\n",
      "24671/24671 [==============================] - 12s 496us/sample - loss: 0.1080 - acc: 0.9588 - val_loss: 0.5002 - val_acc: 0.8691\n",
      "Epoch 40/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.0983 - acc: 0.9613\n",
      "Epoch 00040: val_acc did not improve from 0.88915\n",
      "24671/24671 [==============================] - 12s 497us/sample - loss: 0.0984 - acc: 0.9613 - val_loss: 0.5922 - val_acc: 0.8691\n"
     ]
    }
   ],
   "source": [
    "history_model1 = model_rnn.fit(X_train,\n",
    "                            y_train[\"product_issue\"].values.reshape(-1,1),\n",
    "                            batch_size=batch_size,\n",
    "                            epochs=epochs,\n",
    "                            verbose=1,\n",
    "                            callbacks=callbacks_list,\n",
    "                            validation_data=(X_validation, y_validation[\"product_issue\"].values.reshape(-1,1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remarks\n",
    "\n",
    "Yep, no improvements. Increasing seq size created more noise. Well, lets see if we can get gains when infering multiple categories.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Micro categories models \n",
    "\n",
    "Now we focus on trying to tag micro categories for each macro issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['product_issues_Quality',\n",
       " 'product_issues_Damaged',\n",
       " 'product_issues_Electrical problems',\n",
       " 'product_issues_Missing pieces',\n",
       " 'business_issues_Payment',\n",
       " 'business_issues_Maintenance',\n",
       " 'business_issues_Customer Services',\n",
       " 'business_issues_Delivery',\n",
       " 'business_issues_Online Services']"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train[tag_columns].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_24\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_17 (LSTM)               (None, 256)               365568    \n",
      "_________________________________________________________________\n",
      "dense_91 (Dense)             (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "activation_91 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_68 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_92 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "activation_92 (Activation)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_69 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_93 (Dense)             (None, 9)                 297       \n",
      "_________________________________________________________________\n",
      "activation_93 (Activation)   (None, 9)                 0         \n",
      "=================================================================\n",
      "Total params: 384,393\n",
      "Trainable params: 384,393\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Pre-loading\n",
    "filepath=\"weights-improvement-SeqModel50MultiTag-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "embedding_dim = 100\n",
    "batch_size = 128\n",
    "epochs = 40\n",
    "stack = 3\n",
    "\n",
    "\n",
    "def create_RNN(n_dense=6,\n",
    "               dense_units=16,\n",
    "               max_seq_len=25,\n",
    "               lstm_cell=128,\n",
    "               activation='selu',\n",
    "               dropout=layers.Dropout,\n",
    "               dropout_rate=0.1,\n",
    "               kernel_initializer='lecun_normal',\n",
    "               optimizer='adam',\n",
    "               num_classes=1,\n",
    "               max_words=100):\n",
    "    \n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(layers.LSTM(lstm_cell, input_shape=(max_seq_len, max_words)))\n",
    "    \n",
    "    for i in range(n_dense - 1):\n",
    "        model.add(layers.Dense(dense_units//(2+i*2), kernel_initializer=kernel_initializer))\n",
    "        model.add(layers.Activation(activation))\n",
    "        model.add(dropout(dropout_rate))\n",
    "\n",
    "    model.add(layers.Dense(num_classes))\n",
    "    model.add(layers.Activation('sigmoid'))\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "network1 = {\n",
    "    'n_dense': stack,\n",
    "    \"lstm_cell\": 256,\n",
    "    'dense_units': 128,\n",
    "    \"max_seq_len\":max_seq,\n",
    "    'activation': 'selu',\n",
    "    'dropout': layers.Dropout,\n",
    "    'dropout_rate': 0.5,\n",
    "    \"max_words\": embedding_dim,\n",
    "    'kernel_initializer': 'glorot_uniform',\n",
    "    'optimizer': 'adam'\n",
    "}\n",
    "\n",
    "num_classes = len(y_train[tag_columns].columns)\n",
    "\n",
    "model_rnn = create_RNN(num_classes=num_classes, **network1)\n",
    "model_rnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 24671 samples, validate on 1299 samples\n",
      "Epoch 1/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 1.8090 - acc: 0.4437\n",
      "Epoch 00001: val_acc improved from -inf to 0.51116, saving model to weights-improvement-SeqModel50MultiTag-01-0.51.hdf5\n",
      "24671/24671 [==============================] - 15s 588us/sample - loss: 1.8075 - acc: 0.4439 - val_loss: 1.3380 - val_acc: 0.5112\n",
      "Epoch 2/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 1.4106 - acc: 0.5279\n",
      "Epoch 00002: val_acc improved from 0.51116 to 0.52040, saving model to weights-improvement-SeqModel50MultiTag-02-0.52.hdf5\n",
      "24671/24671 [==============================] - 12s 497us/sample - loss: 1.4105 - acc: 0.5279 - val_loss: 1.2685 - val_acc: 0.5204\n",
      "Epoch 3/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 1.3034 - acc: 0.5626\n",
      "Epoch 00003: val_acc improved from 0.52040 to 0.56120, saving model to weights-improvement-SeqModel50MultiTag-03-0.56.hdf5\n",
      "24671/24671 [==============================] - 12s 498us/sample - loss: 1.3041 - acc: 0.5621 - val_loss: 1.2218 - val_acc: 0.5612\n",
      "Epoch 4/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 1.2300 - acc: 0.5930\n",
      "Epoch 00004: val_acc improved from 0.56120 to 0.61047, saving model to weights-improvement-SeqModel50MultiTag-04-0.61.hdf5\n",
      "24671/24671 [==============================] - 12s 501us/sample - loss: 1.2295 - acc: 0.5928 - val_loss: 1.1941 - val_acc: 0.6105\n",
      "Epoch 5/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 1.1782 - acc: 0.6172\n",
      "Epoch 00005: val_acc improved from 0.61047 to 0.63433, saving model to weights-improvement-SeqModel50MultiTag-05-0.63.hdf5\n",
      "24671/24671 [==============================] - 12s 500us/sample - loss: 1.1780 - acc: 0.6173 - val_loss: 1.1531 - val_acc: 0.6343\n",
      "Epoch 6/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 1.1205 - acc: 0.6347\n",
      "Epoch 00006: val_acc did not improve from 0.63433\n",
      "24671/24671 [==============================] - 12s 505us/sample - loss: 1.1205 - acc: 0.6348 - val_loss: 1.1234 - val_acc: 0.6282\n",
      "Epoch 7/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 1.0756 - acc: 0.6484\n",
      "Epoch 00007: val_acc did not improve from 0.63433\n",
      "24671/24671 [==============================] - 12s 506us/sample - loss: 1.0758 - acc: 0.6483 - val_loss: 1.1306 - val_acc: 0.6305\n",
      "Epoch 8/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 1.0353 - acc: 0.6578\n",
      "Epoch 00008: val_acc did not improve from 0.63433\n",
      "24671/24671 [==============================] - 12s 491us/sample - loss: 1.0356 - acc: 0.6580 - val_loss: 1.1458 - val_acc: 0.6282\n",
      "Epoch 9/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.9845 - acc: 0.6713\n",
      "Epoch 00009: val_acc improved from 0.63433 to 0.64203, saving model to weights-improvement-SeqModel50MultiTag-09-0.64.hdf5\n",
      "24671/24671 [==============================] - 12s 491us/sample - loss: 0.9836 - acc: 0.6717 - val_loss: 1.1888 - val_acc: 0.6420\n",
      "Epoch 10/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.9501 - acc: 0.6825\n",
      "Epoch 00010: val_acc did not improve from 0.64203\n",
      "24671/24671 [==============================] - 12s 503us/sample - loss: 0.9499 - acc: 0.6826 - val_loss: 1.1777 - val_acc: 0.6397\n",
      "Epoch 11/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.9022 - acc: 0.6986\n",
      "Epoch 00011: val_acc improved from 0.64203 to 0.65512, saving model to weights-improvement-SeqModel50MultiTag-11-0.66.hdf5\n",
      "24671/24671 [==============================] - 12s 503us/sample - loss: 0.9022 - acc: 0.6987 - val_loss: 1.1840 - val_acc: 0.6551\n",
      "Epoch 12/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.8605 - acc: 0.7105\n",
      "Epoch 00012: val_acc did not improve from 0.65512\n",
      "24671/24671 [==============================] - 12s 499us/sample - loss: 0.8608 - acc: 0.7103 - val_loss: 1.2470 - val_acc: 0.6351\n",
      "Epoch 13/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.8253 - acc: 0.7217\n",
      "Epoch 00013: val_acc did not improve from 0.65512\n",
      "24671/24671 [==============================] - 12s 501us/sample - loss: 0.8248 - acc: 0.7219 - val_loss: 1.2456 - val_acc: 0.6513\n",
      "Epoch 14/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.7804 - acc: 0.7351\n",
      "Epoch 00014: val_acc did not improve from 0.65512\n",
      "24671/24671 [==============================] - 12s 490us/sample - loss: 0.7803 - acc: 0.7353 - val_loss: 1.3281 - val_acc: 0.6467\n",
      "Epoch 15/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.7497 - acc: 0.7451\n",
      "Epoch 00015: val_acc did not improve from 0.65512\n",
      "24671/24671 [==============================] - 12s 495us/sample - loss: 0.7486 - acc: 0.7455 - val_loss: 1.3374 - val_acc: 0.6374\n",
      "Epoch 16/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.7182 - acc: 0.7577\n",
      "Epoch 00016: val_acc did not improve from 0.65512\n",
      "24671/24671 [==============================] - 12s 494us/sample - loss: 0.7182 - acc: 0.7574 - val_loss: 1.3665 - val_acc: 0.6467\n",
      "Epoch 17/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.6814 - acc: 0.7703\n",
      "Epoch 00017: val_acc did not improve from 0.65512\n",
      "24671/24671 [==============================] - 12s 494us/sample - loss: 0.6815 - acc: 0.7705 - val_loss: 1.4386 - val_acc: 0.6497\n",
      "Epoch 18/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.6584 - acc: 0.7768\n",
      "Epoch 00018: val_acc did not improve from 0.65512\n",
      "24671/24671 [==============================] - 12s 501us/sample - loss: 0.6582 - acc: 0.7769 - val_loss: 1.4788 - val_acc: 0.6451\n",
      "Epoch 19/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.6248 - acc: 0.7887\n",
      "Epoch 00019: val_acc did not improve from 0.65512\n",
      "24671/24671 [==============================] - 12s 487us/sample - loss: 0.6248 - acc: 0.7887 - val_loss: 1.4834 - val_acc: 0.6513\n",
      "Epoch 20/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.5902 - acc: 0.7970\n",
      "Epoch 00020: val_acc did not improve from 0.65512\n",
      "24671/24671 [==============================] - 12s 493us/sample - loss: 0.5907 - acc: 0.7968 - val_loss: 1.5538 - val_acc: 0.6520\n",
      "Epoch 21/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.5850 - acc: 0.8016\n",
      "Epoch 00021: val_acc did not improve from 0.65512\n",
      "24671/24671 [==============================] - 12s 493us/sample - loss: 0.5849 - acc: 0.8018 - val_loss: 1.5978 - val_acc: 0.6320\n",
      "Epoch 22/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.5535 - acc: 0.8101\n",
      "Epoch 00022: val_acc did not improve from 0.65512\n",
      "24671/24671 [==============================] - 12s 499us/sample - loss: 0.5531 - acc: 0.8103 - val_loss: 1.6705 - val_acc: 0.6459\n",
      "Epoch 23/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.5363 - acc: 0.8193\n",
      "Epoch 00023: val_acc did not improve from 0.65512\n",
      "24671/24671 [==============================] - 12s 500us/sample - loss: 0.5364 - acc: 0.8191 - val_loss: 1.6931 - val_acc: 0.6467\n",
      "Epoch 24/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.5210 - acc: 0.8256\n",
      "Epoch 00024: val_acc did not improve from 0.65512\n",
      "24671/24671 [==============================] - 12s 496us/sample - loss: 0.5217 - acc: 0.8254 - val_loss: 1.7731 - val_acc: 0.6390\n",
      "Epoch 25/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.4883 - acc: 0.8361\n",
      "Epoch 00025: val_acc did not improve from 0.65512\n",
      "24671/24671 [==============================] - 12s 500us/sample - loss: 0.4885 - acc: 0.8361 - val_loss: 1.9050 - val_acc: 0.6374\n",
      "Epoch 26/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.4730 - acc: 0.8429\n",
      "Epoch 00026: val_acc did not improve from 0.65512\n",
      "24671/24671 [==============================] - 12s 500us/sample - loss: 0.4732 - acc: 0.8428 - val_loss: 1.8569 - val_acc: 0.6374\n",
      "Epoch 27/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.4663 - acc: 0.8483\n",
      "Epoch 00027: val_acc did not improve from 0.65512\n",
      "24671/24671 [==============================] - 12s 501us/sample - loss: 0.4665 - acc: 0.8482 - val_loss: 1.9763 - val_acc: 0.6420\n",
      "Epoch 28/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.4543 - acc: 0.8538\n",
      "Epoch 00028: val_acc did not improve from 0.65512\n",
      "24671/24671 [==============================] - 12s 501us/sample - loss: 0.4543 - acc: 0.8537 - val_loss: 1.9703 - val_acc: 0.6343\n",
      "Epoch 29/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.4447 - acc: 0.8586\n",
      "Epoch 00029: val_acc did not improve from 0.65512\n",
      "24671/24671 [==============================] - 12s 500us/sample - loss: 0.4441 - acc: 0.8588 - val_loss: 2.0005 - val_acc: 0.6174\n",
      "Epoch 30/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.4224 - acc: 0.8691\n",
      "Epoch 00030: val_acc did not improve from 0.65512\n",
      "24671/24671 [==============================] - 12s 504us/sample - loss: 0.4225 - acc: 0.8690 - val_loss: 2.0395 - val_acc: 0.6236\n",
      "Epoch 31/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.4161 - acc: 0.8726\n",
      "Epoch 00031: val_acc did not improve from 0.65512\n",
      "24671/24671 [==============================] - 12s 501us/sample - loss: 0.4164 - acc: 0.8724 - val_loss: 2.0950 - val_acc: 0.6328\n",
      "Epoch 32/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.4187 - acc: 0.8741\n",
      "Epoch 00032: val_acc did not improve from 0.65512\n",
      "24671/24671 [==============================] - 12s 502us/sample - loss: 0.4182 - acc: 0.8743 - val_loss: 1.9975 - val_acc: 0.6259\n",
      "Epoch 33/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.3999 - acc: 0.8836\n",
      "Epoch 00033: val_acc did not improve from 0.65512\n",
      "24671/24671 [==============================] - 12s 484us/sample - loss: 0.3999 - acc: 0.8836 - val_loss: 2.2527 - val_acc: 0.6212\n",
      "Epoch 34/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.3755 - acc: 0.8921\n",
      "Epoch 00034: val_acc did not improve from 0.65512\n",
      "24671/24671 [==============================] - 12s 497us/sample - loss: 0.3757 - acc: 0.8921 - val_loss: 2.2004 - val_acc: 0.6220\n",
      "Epoch 35/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.3461 - acc: 0.9019\n",
      "Epoch 00035: val_acc did not improve from 0.65512\n",
      "24671/24671 [==============================] - 12s 503us/sample - loss: 0.3458 - acc: 0.9019 - val_loss: 2.4148 - val_acc: 0.6313\n",
      "Epoch 36/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.3547 - acc: 0.8990\n",
      "Epoch 00036: val_acc did not improve from 0.65512\n",
      "24671/24671 [==============================] - 12s 506us/sample - loss: 0.3552 - acc: 0.8989 - val_loss: 2.4261 - val_acc: 0.6351\n",
      "Epoch 37/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.3381 - acc: 0.9028\n",
      "Epoch 00037: val_acc did not improve from 0.65512\n",
      "24671/24671 [==============================] - 12s 504us/sample - loss: 0.3384 - acc: 0.9028 - val_loss: 2.5015 - val_acc: 0.6313\n",
      "Epoch 38/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.3498 - acc: 0.8990\n",
      "Epoch 00038: val_acc did not improve from 0.65512\n",
      "24671/24671 [==============================] - 12s 503us/sample - loss: 0.3504 - acc: 0.8990 - val_loss: 2.5280 - val_acc: 0.6182\n",
      "Epoch 39/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.3391 - acc: 0.9042\n",
      "Epoch 00039: val_acc did not improve from 0.65512\n",
      "24671/24671 [==============================] - 12s 500us/sample - loss: 0.3391 - acc: 0.9042 - val_loss: 2.4812 - val_acc: 0.6266\n",
      "Epoch 40/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.3289 - acc: 0.9084\n",
      "Epoch 00040: val_acc did not improve from 0.65512\n",
      "24671/24671 [==============================] - 12s 499us/sample - loss: 0.3286 - acc: 0.9083 - val_loss: 2.6683 - val_acc: 0.6251\n"
     ]
    }
   ],
   "source": [
    "history_model1 = model_rnn.fit(X_train,\n",
    "                            y_train[tag_columns],\n",
    "                            batch_size=batch_size,\n",
    "                            epochs=epochs,\n",
    "                            verbose=1,\n",
    "                            callbacks=callbacks_list,\n",
    "                            validation_data=(X_validation, y_validation[tag_columns]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remarks\n",
    "\n",
    "Well, nice improvement from our previous iteration.\n",
    "\n",
    "Lets break, into simpler two models. Lets see if we have a reasonable improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_25\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_18 (LSTM)               (None, 128)               117248    \n",
      "_________________________________________________________________\n",
      "dense_94 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "activation_94 (Activation)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_70 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_95 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "activation_95 (Activation)   (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dropout_71 (Dropout)         (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_96 (Dense)             (None, 4)                 68        \n",
      "_________________________________________________________________\n",
      "activation_96 (Activation)   (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 121,972\n",
      "Trainable params: 121,972\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Pre-loading\n",
    "filepath=\"weights-improvement-SeqModel50MultiTag-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "embedding_dim = 100\n",
    "batch_size = 128\n",
    "epochs = 40\n",
    "stack = 3\n",
    "\n",
    "\n",
    "def create_RNN(n_dense=6,\n",
    "               dense_units=16,\n",
    "               max_seq_len=25,\n",
    "               lstm_cell=128,\n",
    "               activation='selu',\n",
    "               dropout=layers.Dropout,\n",
    "               dropout_rate=0.1,\n",
    "               kernel_initializer='lecun_normal',\n",
    "               optimizer='adam',\n",
    "               num_classes=1,\n",
    "               max_words=100):\n",
    "    \n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(layers.LSTM(lstm_cell, input_shape=(max_seq_len, max_words)))\n",
    "    \n",
    "    for i in range(n_dense - 1):\n",
    "        model.add(layers.Dense(dense_units//(2+i*2), kernel_initializer=kernel_initializer))\n",
    "        model.add(layers.Activation(activation))\n",
    "        model.add(dropout(dropout_rate))\n",
    "\n",
    "    model.add(layers.Dense(num_classes))\n",
    "    model.add(layers.Activation('sigmoid'))\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "network1 = {\n",
    "    'n_dense': stack,\n",
    "    \"lstm_cell\": 128,\n",
    "    'dense_units': 64,\n",
    "    \"max_seq_len\":max_seq,\n",
    "    'activation': 'selu',\n",
    "    'dropout': layers.Dropout,\n",
    "    'dropout_rate': 0.5,\n",
    "    \"max_words\": embedding_dim,\n",
    "    'kernel_initializer': 'glorot_uniform',\n",
    "    'optimizer': 'adam'\n",
    "}\n",
    "\n",
    "num_classes = len(y_train[product_issues_columns].columns)\n",
    "\n",
    "model_rnn_prod_tags = create_RNN(num_classes=num_classes, **network1)\n",
    "model_rnn_prod_tags.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 24671 samples, validate on 1299 samples\n",
      "Epoch 1/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.2450 - acc: 0.6028\n",
      "Epoch 00001: val_acc improved from -inf to 0.91455, saving model to weights-improvement-SeqModel50MultiTag-01-0.91.hdf5\n",
      "24671/24671 [==============================] - 15s 613us/sample - loss: 0.2449 - acc: 0.6034 - val_loss: 0.2013 - val_acc: 0.9145\n",
      "Epoch 2/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.2170 - acc: 0.7950\n",
      "Epoch 00002: val_acc improved from 0.91455 to 0.91686, saving model to weights-improvement-SeqModel50MultiTag-02-0.92.hdf5\n",
      "24671/24671 [==============================] - 12s 505us/sample - loss: 0.2169 - acc: 0.7949 - val_loss: 0.1970 - val_acc: 0.9169\n",
      "Epoch 3/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.2081 - acc: 0.8606\n",
      "Epoch 00003: val_acc did not improve from 0.91686\n",
      "24671/24671 [==============================] - 13s 512us/sample - loss: 0.2088 - acc: 0.8606 - val_loss: 0.1969 - val_acc: 0.9130\n",
      "Epoch 4/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.2010 - acc: 0.8824\n",
      "Epoch 00004: val_acc did not improve from 0.91686\n",
      "24671/24671 [==============================] - 12s 500us/sample - loss: 0.2010 - acc: 0.8825 - val_loss: 0.1942 - val_acc: 0.9122\n",
      "Epoch 5/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.1896 - acc: 0.8879\n",
      "Epoch 00005: val_acc did not improve from 0.91686\n",
      "24671/24671 [==============================] - 12s 506us/sample - loss: 0.1898 - acc: 0.8879 - val_loss: 0.2035 - val_acc: 0.8938\n",
      "Epoch 6/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.1810 - acc: 0.8794\n",
      "Epoch 00006: val_acc did not improve from 0.91686\n",
      "24671/24671 [==============================] - 12s 506us/sample - loss: 0.1811 - acc: 0.8796 - val_loss: 0.2030 - val_acc: 0.8976\n",
      "Epoch 7/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.1712 - acc: 0.8795\n",
      "Epoch 00007: val_acc did not improve from 0.91686\n",
      "24671/24671 [==============================] - 12s 502us/sample - loss: 0.1711 - acc: 0.8793 - val_loss: 0.2123 - val_acc: 0.8761\n",
      "Epoch 8/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.1605 - acc: 0.8756\n",
      "Epoch 00008: val_acc did not improve from 0.91686\n",
      "24671/24671 [==============================] - 12s 501us/sample - loss: 0.1607 - acc: 0.8755 - val_loss: 0.2083 - val_acc: 0.8876\n",
      "Epoch 9/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.1523 - acc: 0.8568\n",
      "Epoch 00009: val_acc did not improve from 0.91686\n",
      "24671/24671 [==============================] - 12s 491us/sample - loss: 0.1523 - acc: 0.8568 - val_loss: 0.2243 - val_acc: 0.8784\n",
      "Epoch 10/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.1463 - acc: 0.8530\n",
      "Epoch 00010: val_acc did not improve from 0.91686\n",
      "24671/24671 [==============================] - 12s 499us/sample - loss: 0.1462 - acc: 0.8531 - val_loss: 0.2260 - val_acc: 0.8491\n",
      "Epoch 11/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.1389 - acc: 0.8387\n",
      "Epoch 00011: val_acc did not improve from 0.91686\n",
      "24671/24671 [==============================] - 13s 507us/sample - loss: 0.1389 - acc: 0.8390 - val_loss: 0.2484 - val_acc: 0.8814\n",
      "Epoch 12/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.1258 - acc: 0.8254\n",
      "Epoch 00012: val_acc did not improve from 0.91686\n",
      "24671/24671 [==============================] - 13s 516us/sample - loss: 0.1258 - acc: 0.8253 - val_loss: 0.2645 - val_acc: 0.7744\n",
      "Epoch 13/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.1252 - acc: 0.7920\n",
      "Epoch 00013: val_acc did not improve from 0.91686\n",
      "24671/24671 [==============================] - 13s 508us/sample - loss: 0.1253 - acc: 0.7920 - val_loss: 0.2678 - val_acc: 0.8037\n",
      "Epoch 14/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.1131 - acc: 0.7865\n",
      "Epoch 00014: val_acc did not improve from 0.91686\n",
      "24671/24671 [==============================] - 12s 506us/sample - loss: 0.1132 - acc: 0.7865 - val_loss: 0.2906 - val_acc: 0.7691\n",
      "Epoch 15/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.1072 - acc: 0.7538\n",
      "Epoch 00015: val_acc did not improve from 0.91686\n",
      "24671/24671 [==============================] - 13s 510us/sample - loss: 0.1072 - acc: 0.7536 - val_loss: 0.3015 - val_acc: 0.7036\n",
      "Epoch 16/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.1009 - acc: 0.7523\n",
      "Epoch 00016: val_acc did not improve from 0.91686\n",
      "24671/24671 [==============================] - 13s 514us/sample - loss: 0.1010 - acc: 0.7523 - val_loss: 0.3113 - val_acc: 0.6759\n",
      "Epoch 17/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.0908 - acc: 0.7377\n",
      "Epoch 00017: val_acc did not improve from 0.91686\n",
      "24671/24671 [==============================] - 13s 509us/sample - loss: 0.0908 - acc: 0.7377 - val_loss: 0.3143 - val_acc: 0.6282\n",
      "Epoch 18/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.0763 - acc: 0.7279\n",
      "Epoch 00018: val_acc did not improve from 0.91686\n",
      "24671/24671 [==============================] - 13s 515us/sample - loss: 0.0763 - acc: 0.7279 - val_loss: 0.3953 - val_acc: 0.7075\n",
      "Epoch 19/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.0717 - acc: 0.7296\n",
      "Epoch 00019: val_acc did not improve from 0.91686\n",
      "24671/24671 [==============================] - 13s 511us/sample - loss: 0.0715 - acc: 0.7293 - val_loss: 0.3771 - val_acc: 0.6505\n",
      "Epoch 20/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.0682 - acc: 0.7147\n",
      "Epoch 00020: val_acc did not improve from 0.91686\n",
      "24671/24671 [==============================] - 13s 512us/sample - loss: 0.0682 - acc: 0.7150 - val_loss: 0.4062 - val_acc: 0.6674\n",
      "Epoch 21/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.0597 - acc: 0.7297\n",
      "Epoch 00021: val_acc did not improve from 0.91686\n",
      "24671/24671 [==============================] - 13s 508us/sample - loss: 0.0597 - acc: 0.7299 - val_loss: 0.4068 - val_acc: 0.7013\n",
      "Epoch 22/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.0517 - acc: 0.7187\n",
      "Epoch 00022: val_acc did not improve from 0.91686\n",
      "24671/24671 [==============================] - 13s 509us/sample - loss: 0.0518 - acc: 0.7187 - val_loss: 0.4755 - val_acc: 0.6243\n",
      "Epoch 23/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.0524 - acc: 0.7270\n",
      "Epoch 00023: val_acc did not improve from 0.91686\n",
      "24671/24671 [==============================] - 13s 512us/sample - loss: 0.0523 - acc: 0.7269 - val_loss: 0.4596 - val_acc: 0.6667\n",
      "Epoch 24/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.0505 - acc: 0.7235\n",
      "Epoch 00024: val_acc did not improve from 0.91686\n",
      "24671/24671 [==============================] - 13s 513us/sample - loss: 0.0504 - acc: 0.7232 - val_loss: 0.5271 - val_acc: 0.6089\n",
      "Epoch 25/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.0489 - acc: 0.7216\n",
      "Epoch 00025: val_acc did not improve from 0.91686\n",
      "24671/24671 [==============================] - 13s 510us/sample - loss: 0.0492 - acc: 0.7217 - val_loss: 0.5087 - val_acc: 0.6343\n",
      "Epoch 26/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.0439 - acc: 0.7225\n",
      "Epoch 00026: val_acc did not improve from 0.91686\n",
      "24671/24671 [==============================] - 13s 515us/sample - loss: 0.0439 - acc: 0.7225 - val_loss: 0.5286 - val_acc: 0.6443\n",
      "Epoch 27/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.0394 - acc: 0.7296\n",
      "Epoch 00027: val_acc did not improve from 0.91686\n",
      "24671/24671 [==============================] - 13s 513us/sample - loss: 0.0395 - acc: 0.7298 - val_loss: 0.5522 - val_acc: 0.6359\n",
      "Epoch 28/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.0415 - acc: 0.7215\n",
      "Epoch 00028: val_acc did not improve from 0.91686\n",
      "24671/24671 [==============================] - 12s 505us/sample - loss: 0.0415 - acc: 0.7212 - val_loss: 0.5580 - val_acc: 0.6436\n",
      "Epoch 29/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.0422 - acc: 0.7233\n",
      "Epoch 00029: val_acc did not improve from 0.91686\n",
      "24671/24671 [==============================] - 12s 497us/sample - loss: 0.0422 - acc: 0.7234 - val_loss: 0.5307 - val_acc: 0.6636\n",
      "Epoch 30/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.0417 - acc: 0.7300\n",
      "Epoch 00030: val_acc did not improve from 0.91686\n",
      "24671/24671 [==============================] - 13s 513us/sample - loss: 0.0417 - acc: 0.7300 - val_loss: 0.5547 - val_acc: 0.6705\n",
      "Epoch 31/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.0351 - acc: 0.7352\n",
      "Epoch 00031: val_acc did not improve from 0.91686\n",
      "24671/24671 [==============================] - 13s 514us/sample - loss: 0.0352 - acc: 0.7349 - val_loss: 0.5965 - val_acc: 0.6251\n",
      "Epoch 32/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.0394 - acc: 0.7288\n",
      "Epoch 00032: val_acc did not improve from 0.91686\n",
      "24671/24671 [==============================] - 13s 507us/sample - loss: 0.0398 - acc: 0.7287 - val_loss: 0.5868 - val_acc: 0.6644\n",
      "Epoch 33/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.0325 - acc: 0.7356\n",
      "Epoch 00033: val_acc did not improve from 0.91686\n",
      "24671/24671 [==============================] - 13s 508us/sample - loss: 0.0326 - acc: 0.7358 - val_loss: 0.6102 - val_acc: 0.7021\n",
      "Epoch 34/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.0318 - acc: 0.7330\n",
      "Epoch 00034: val_acc did not improve from 0.91686\n",
      "24671/24671 [==============================] - 13s 513us/sample - loss: 0.0317 - acc: 0.7328 - val_loss: 0.6225 - val_acc: 0.6505\n",
      "Epoch 35/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.0312 - acc: 0.7271\n",
      "Epoch 00035: val_acc did not improve from 0.91686\n",
      "24671/24671 [==============================] - 13s 515us/sample - loss: 0.0311 - acc: 0.7271 - val_loss: 0.6151 - val_acc: 0.6543\n",
      "Epoch 36/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.0314 - acc: 0.7282\n",
      "Epoch 00036: val_acc did not improve from 0.91686\n",
      "24671/24671 [==============================] - 13s 509us/sample - loss: 0.0314 - acc: 0.7281 - val_loss: 0.6661 - val_acc: 0.6713\n",
      "Epoch 37/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.0371 - acc: 0.7233\n",
      "Epoch 00037: val_acc did not improve from 0.91686\n",
      "24671/24671 [==============================] - 13s 511us/sample - loss: 0.0371 - acc: 0.7233 - val_loss: 0.6055 - val_acc: 0.6359\n",
      "Epoch 38/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.0325 - acc: 0.7102\n",
      "Epoch 00038: val_acc did not improve from 0.91686\n",
      "24671/24671 [==============================] - 13s 512us/sample - loss: 0.0324 - acc: 0.7103 - val_loss: 0.6350 - val_acc: 0.6405\n",
      "Epoch 39/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.0334 - acc: 0.7107\n",
      "Epoch 00039: val_acc did not improve from 0.91686\n",
      "24671/24671 [==============================] - 13s 518us/sample - loss: 0.0333 - acc: 0.7108 - val_loss: 0.6311 - val_acc: 0.6297\n",
      "Epoch 40/40\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.0394 - acc: 0.7193\n",
      "Epoch 00040: val_acc did not improve from 0.91686\n",
      "24671/24671 [==============================] - 13s 511us/sample - loss: 0.0393 - acc: 0.7191 - val_loss: 0.6030 - val_acc: 0.6443\n"
     ]
    }
   ],
   "source": [
    "history_model1 = model_rnn_prod_tags.fit(X_train,\n",
    "                            y_train[product_issues_columns],\n",
    "                            batch_size=batch_size,\n",
    "                            epochs=epochs,\n",
    "                            verbose=1,\n",
    "                            callbacks=callbacks_list,\n",
    "                            validation_data=(X_validation, y_validation[product_issues_columns]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business issue tags\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_26\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_19 (LSTM)               (None, 128)               117248    \n",
      "_________________________________________________________________\n",
      "dense_97 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "activation_97 (Activation)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_72 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_98 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "activation_98 (Activation)   (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dropout_73 (Dropout)         (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_99 (Dense)             (None, 5)                 85        \n",
      "_________________________________________________________________\n",
      "activation_99 (Activation)   (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 121,989\n",
      "Trainable params: 121,989\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Pre-loading\n",
    "filepath=\"weights-improvement-SeqModel50MultiTagBusiness-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "embedding_dim = 100\n",
    "batch_size = 128\n",
    "epochs = 20\n",
    "stack = 3\n",
    "\n",
    "\n",
    "def create_RNN(n_dense=6,\n",
    "               dense_units=16,\n",
    "               max_seq_len=25,\n",
    "               lstm_cell=128,\n",
    "               activation='selu',\n",
    "               dropout=layers.Dropout,\n",
    "               dropout_rate=0.1,\n",
    "               kernel_initializer='lecun_normal',\n",
    "               optimizer='adam',\n",
    "               num_classes=1,\n",
    "               max_words=100):\n",
    "    \n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(layers.LSTM(lstm_cell, input_shape=(max_seq_len, max_words)))\n",
    "    \n",
    "    for i in range(n_dense - 1):\n",
    "        model.add(layers.Dense(dense_units//(2+i*2), kernel_initializer=kernel_initializer))\n",
    "        model.add(layers.Activation(activation))\n",
    "        model.add(dropout(dropout_rate))\n",
    "\n",
    "    model.add(layers.Dense(num_classes))\n",
    "    model.add(layers.Activation('sigmoid'))\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "network2 = {\n",
    "    'n_dense': stack,\n",
    "    \"lstm_cell\": 128,\n",
    "    'dense_units': 64,\n",
    "    \"max_seq_len\":max_seq,\n",
    "    'activation': 'selu',\n",
    "    'dropout': layers.Dropout,\n",
    "    'dropout_rate': 0.5,\n",
    "    \"max_words\": embedding_dim,\n",
    "    'kernel_initializer': 'glorot_uniform',\n",
    "    'optimizer': 'adam'\n",
    "}\n",
    "\n",
    "num_classes = len(y_train[business_issues_columns].columns)\n",
    "\n",
    "model_rnn_business_tags = create_RNN(num_classes=num_classes, **network2)\n",
    "model_rnn_business_tags.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 24671 samples, validate on 1299 samples\n",
      "Epoch 1/20\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 1.0519 - acc: 0.4460\n",
      "Epoch 00001: val_acc improved from -inf to 0.45958, saving model to weights-improvement-SeqModel50MultiTagBusiness-01-0.46.hdf5\n",
      "24671/24671 [==============================] - 15s 621us/sample - loss: 1.0515 - acc: 0.4458 - val_loss: 0.9036 - val_acc: 0.4596\n",
      "Epoch 2/20\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.9076 - acc: 0.4926\n",
      "Epoch 00002: val_acc improved from 0.45958 to 0.47421, saving model to weights-improvement-SeqModel50MultiTagBusiness-02-0.47.hdf5\n",
      "24671/24671 [==============================] - 12s 495us/sample - loss: 0.9070 - acc: 0.4928 - val_loss: 0.7965 - val_acc: 0.4742\n",
      "Epoch 3/20\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.8301 - acc: 0.5087\n",
      "Epoch 00003: val_acc improved from 0.47421 to 0.50500, saving model to weights-improvement-SeqModel50MultiTagBusiness-03-0.51.hdf5\n",
      "24671/24671 [==============================] - 12s 492us/sample - loss: 0.8298 - acc: 0.5090 - val_loss: 0.7531 - val_acc: 0.5050\n",
      "Epoch 4/20\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.7942 - acc: 0.5254\n",
      "Epoch 00004: val_acc improved from 0.50500 to 0.51270, saving model to weights-improvement-SeqModel50MultiTagBusiness-04-0.51.hdf5\n",
      "24671/24671 [==============================] - 12s 494us/sample - loss: 0.7945 - acc: 0.5253 - val_loss: 0.7478 - val_acc: 0.5127\n",
      "Epoch 5/20\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.7624 - acc: 0.5361\n",
      "Epoch 00005: val_acc improved from 0.51270 to 0.57275, saving model to weights-improvement-SeqModel50MultiTagBusiness-05-0.57.hdf5\n",
      "24671/24671 [==============================] - 12s 493us/sample - loss: 0.7621 - acc: 0.5363 - val_loss: 0.7176 - val_acc: 0.5727\n",
      "Epoch 6/20\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.7382 - acc: 0.5485\n",
      "Epoch 00006: val_acc did not improve from 0.57275\n",
      "24671/24671 [==============================] - 12s 493us/sample - loss: 0.7384 - acc: 0.5483 - val_loss: 0.7075 - val_acc: 0.5443\n",
      "Epoch 7/20\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.7180 - acc: 0.5557\n",
      "Epoch 00007: val_acc did not improve from 0.57275\n",
      "24671/24671 [==============================] - 12s 500us/sample - loss: 0.7180 - acc: 0.5556 - val_loss: 0.6895 - val_acc: 0.5666\n",
      "Epoch 8/20\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.6943 - acc: 0.5683\n",
      "Epoch 00008: val_acc improved from 0.57275 to 0.57429, saving model to weights-improvement-SeqModel50MultiTagBusiness-08-0.57.hdf5\n",
      "24671/24671 [==============================] - 12s 502us/sample - loss: 0.6949 - acc: 0.5685 - val_loss: 0.6883 - val_acc: 0.5743\n",
      "Epoch 9/20\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.6752 - acc: 0.5734\n",
      "Epoch 00009: val_acc improved from 0.57429 to 0.57968, saving model to weights-improvement-SeqModel50MultiTagBusiness-09-0.58.hdf5\n",
      "24671/24671 [==============================] - 12s 491us/sample - loss: 0.6757 - acc: 0.5732 - val_loss: 0.6852 - val_acc: 0.5797\n",
      "Epoch 10/20\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.6483 - acc: 0.5789\n",
      "Epoch 00010: val_acc did not improve from 0.57968\n",
      "24671/24671 [==============================] - 12s 487us/sample - loss: 0.6480 - acc: 0.5789 - val_loss: 0.6771 - val_acc: 0.5781\n",
      "Epoch 11/20\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.6391 - acc: 0.5811\n",
      "Epoch 00011: val_acc did not improve from 0.57968\n",
      "24671/24671 [==============================] - 12s 501us/sample - loss: 0.6387 - acc: 0.5812 - val_loss: 0.6784 - val_acc: 0.5797\n",
      "Epoch 12/20\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.6110 - acc: 0.5952\n",
      "Epoch 00012: val_acc did not improve from 0.57968\n",
      "24671/24671 [==============================] - 12s 497us/sample - loss: 0.6111 - acc: 0.5952 - val_loss: 0.6735 - val_acc: 0.5797\n",
      "Epoch 13/20\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.5929 - acc: 0.5973\n",
      "Epoch 00013: val_acc did not improve from 0.57968\n",
      "24671/24671 [==============================] - 12s 491us/sample - loss: 0.5935 - acc: 0.5974 - val_loss: 0.6973 - val_acc: 0.5758\n",
      "Epoch 14/20\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.5722 - acc: 0.6075\n",
      "Epoch 00014: val_acc did not improve from 0.57968\n",
      "24671/24671 [==============================] - 12s 496us/sample - loss: 0.5721 - acc: 0.6076 - val_loss: 0.6940 - val_acc: 0.5789\n",
      "Epoch 15/20\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.5567 - acc: 0.6145\n",
      "Epoch 00015: val_acc did not improve from 0.57968\n",
      "24671/24671 [==============================] - 12s 498us/sample - loss: 0.5566 - acc: 0.6144 - val_loss: 0.6986 - val_acc: 0.5758\n",
      "Epoch 16/20\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.5417 - acc: 0.6182\n",
      "Epoch 00016: val_acc improved from 0.57968 to 0.58276, saving model to weights-improvement-SeqModel50MultiTagBusiness-16-0.58.hdf5\n",
      "24671/24671 [==============================] - 12s 487us/sample - loss: 0.5415 - acc: 0.6183 - val_loss: 0.7106 - val_acc: 0.5828\n",
      "Epoch 17/20\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.5252 - acc: 0.6229\n",
      "Epoch 00017: val_acc did not improve from 0.58276\n",
      "24671/24671 [==============================] - 12s 496us/sample - loss: 0.5251 - acc: 0.6230 - val_loss: 0.7102 - val_acc: 0.5828\n",
      "Epoch 18/20\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.4953 - acc: 0.6361\n",
      "Epoch 00018: val_acc improved from 0.58276 to 0.58584, saving model to weights-improvement-SeqModel50MultiTagBusiness-18-0.59.hdf5\n",
      "24671/24671 [==============================] - 12s 505us/sample - loss: 0.4953 - acc: 0.6362 - val_loss: 0.7217 - val_acc: 0.5858\n",
      "Epoch 19/20\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.4852 - acc: 0.6363\n",
      "Epoch 00019: val_acc did not improve from 0.58584\n",
      "24671/24671 [==============================] - 12s 498us/sample - loss: 0.4847 - acc: 0.6363 - val_loss: 0.7298 - val_acc: 0.5858\n",
      "Epoch 20/20\n",
      "24576/24671 [============================>.] - ETA: 0s - loss: 0.4666 - acc: 0.6442\n",
      "Epoch 00020: val_acc did not improve from 0.58584\n",
      "24671/24671 [==============================] - 12s 494us/sample - loss: 0.4663 - acc: 0.6442 - val_loss: 0.7356 - val_acc: 0.5851\n"
     ]
    }
   ],
   "source": [
    "history_model2 = model_rnn_business_tags.fit(X_train,\n",
    "                            y_train[business_issues_columns],\n",
    "                            batch_size=batch_size,\n",
    "                            epochs=epochs,\n",
    "                            verbose=1,\n",
    "                            callbacks=callbacks_list,\n",
    "                            validation_data=(X_validation, y_validation[business_issues_columns]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remarks\n",
    "yeah, classification for business issues is harder than for product issues.\n",
    "\n",
    "Breaking them was helpful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taggging Negative reviews from GB\n",
    "\n",
    "We will use the previous tagged reviews and enrich them using our models predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_review_embbedings_seq(review_seq, model: \"gensim.model\", seq_max=25):\n",
    "    final_seq = np.zeros([seq_max,model.vector_size])\n",
    "    for i,w in enumerate(review_seq):\n",
    "        if i<seq_max:\n",
    "            try:\n",
    "                final_seq[i,:] = model.wv[w]\n",
    "            except Exception as err:\n",
    "                print(err)\n",
    "    return final_seq\n",
    "\n",
    "max_seq = 50\n",
    "toks = get_tokens_from_GB(neg_reviews)  # Now using neg reviews from GB!\n",
    "feat_seq_GB = np.array([get_review_embbedings_seq(token, model, seq_max=max_seq) for token in toks])\n",
    "\n",
    "# Sanity check\n",
    "assert feat_seq_GB.shape == (len(toks), max_seq, model.vector_size)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id_review': 'ChZDSUhNMG9nS0VJQ0FnSURNNXBERUFREAEaFjZwVEFadG8xblgySHZTdEdrMEJlYmc',\n",
       " 'caption': 'Vende eletros domÃ©sticos quebrado.  Comprei um liquidificador com defeito.',\n",
       " 'relative_date': '3 meses atrÃ¡s',\n",
       " 'retrieval_date': '2020-04-15T05:33:14Z',\n",
       " 'rating': 2.0,\n",
       " 'username': 'Mercia Oliveira',\n",
       " 'n_review_user': 39,\n",
       " 'n_photo_user': 0,\n",
       " 'url_user': 'https://www.google.com/maps/contrib/107839935361960381477?hl=pt-BR',\n",
       " 'geo_location': {'lat': '-23.5975251', 'long': '-46.6025457'},\n",
       " 'store': 'magazine-luiza',\n",
       " 'issue': 'product_issue'}"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_reviews[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for review, feat_seq in zip(neg_reviews, feat_seq_GB):\n",
    "    if \"issue\" in review:\n",
    "        if review[\"issue\"] == \"product_issue\":\n",
    "            pred = model_rnn_prod_tags.predict(np.expand_dims(feat_seq, axis=0))\n",
    "            tags = np.array(product_issues_columns)[(np.round(pred)==1)[0]]\n",
    "            review[\"tags\"] = list(tags)\n",
    "        else:\n",
    "            pred = model_rnn_business_tags.predict(np.expand_dims(feat_seq, axis=0))\n",
    "            tags = np.array(business_issues_columns)[(np.round(pred)==1)[0]]\n",
    "            review[\"tags\"] = list(tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id_review': 'ChZDSUhNMG9nS0VJQ0FnSURNNXBERUFREAEaFjZwVEFadG8xblgySHZTdEdrMEJlYmc',\n",
       " 'caption': 'Vende eletros domÃ©sticos quebrado.  Comprei um liquidificador com defeito.',\n",
       " 'relative_date': '3 meses atrÃ¡s',\n",
       " 'retrieval_date': '2020-04-15T05:33:14Z',\n",
       " 'rating': 2.0,\n",
       " 'username': 'Mercia Oliveira',\n",
       " 'n_review_user': 39,\n",
       " 'n_photo_user': 0,\n",
       " 'url_user': 'https://www.google.com/maps/contrib/107839935361960381477?hl=pt-BR',\n",
       " 'geo_location': {'lat': '-23.5975251', 'long': '-46.6025457'},\n",
       " 'store': 'magazine-luiza',\n",
       " 'issue': 'product_issue',\n",
       " 'tags': ['product_issues_Quality', 'product_issues_Damaged']}"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_reviews[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'caption': 'Vende eletros domÃ©sticos quebrado.  Comprei um liquidificador com '\n",
      "            'defeito.',\n",
      " 'geo_location': {'lat': '-23.5975251', 'long': '-46.6025457'},\n",
      " 'id_review': 'ChZDSUhNMG9nS0VJQ0FnSURNNXBERUFREAEaFjZwVEFadG8xblgySHZTdEdrMEJlYmc',\n",
      " 'issue': 'product_issue',\n",
      " 'n_photo_user': 0,\n",
      " 'n_review_user': 39,\n",
      " 'rating': 2.0,\n",
      " 'relative_date': '3 meses atrÃ¡s',\n",
      " 'retrieval_date': '2020-04-15T05:33:14Z',\n",
      " 'store': 'magazine-luiza',\n",
      " 'tags': ['product_issues_Quality', 'product_issues_Damaged'],\n",
      " 'url_user': 'https://www.google.com/maps/contrib/107839935361960381477?hl=pt-BR',\n",
      " 'username': 'Mercia Oliveira'}\n",
      "{'caption': 'PÃ©ssimo atendimento e nÃ£o tem telefone para atender os clientes '\n",
      "            'tem que ir na loja minha primeira compra e Ãºltima nÃ£o dÃ£o '\n",
      "            'satisfaÃ§Ã£o da entrega pqp',\n",
      " 'geo_location': {'lat': '-23.5975251', 'long': '-46.6025457'},\n",
      " 'id_review': 'ChZDSUhNMG9nS0VJQ0FnSUNNdDVidE5nEAEaFmE3R1pMTGtPdmJIUU9tQXYxd2xqVGc',\n",
      " 'issue': 'business_issue',\n",
      " 'n_photo_user': 0,\n",
      " 'n_review_user': 14,\n",
      " 'rating': 1.0,\n",
      " 'relative_date': '4 meses atrÃ¡s',\n",
      " 'retrieval_date': '2020-04-15T05:33:21Z',\n",
      " 'store': 'magazine-luiza',\n",
      " 'tags': ['business_issues_Delivery'],\n",
      " 'url_user': 'https://www.google.com/maps/contrib/113640391183338402739?hl=pt-BR',\n",
      " 'username': 'Gilberto Alves'}\n",
      "{'caption': 'PÃ©ssima..  Pessoas descompromissadas...me impediram do meu '\n",
      "            'direito.de retirar meu televisor pago...simplesmente fecharam a '\n",
      "            'loja ANTES do horÃ¡rio anunciado ...e ainda tiveram o desplante de '\n",
      "            'dizer que era Normal e ainda fui.ameacado de processo pela '\n",
      "            'funcionÃ¡ria do caixa que foi extremamente deselegante e sem '\n",
      "            'educaÃ§Ã£o.  Tenho documentado os nomes de todos estes atores e jÃ¡ '\n",
      "            'descobri o nome do supervisor regional. Fujam desta empresa',\n",
      " 'geo_location': {'lat': '-23.5975251', 'long': '-46.6025457'},\n",
      " 'id_review': 'ChdDSUhNMG9nS0VJQ0FnSUQwdE5QTzVRRRABGhZVOThjbXNEdHBialgwU1JFN3EyQTZB',\n",
      " 'issue': 'business_issue',\n",
      " 'n_photo_user': 0,\n",
      " 'n_review_user': 1,\n",
      " 'rating': 1.0,\n",
      " 'relative_date': '5 meses atrÃ¡s',\n",
      " 'retrieval_date': '2020-04-15T05:33:41Z',\n",
      " 'store': 'magazine-luiza',\n",
      " 'tags': ['business_issues_Customer Services'],\n",
      " 'url_user': 'https://www.google.com/maps/contrib/107424083193977237669?hl=pt-BR',\n",
      " 'username': 'Anderson Freire Carniel'}\n",
      "{'caption': 'Compra de mÃ³veis planejados nunca mais, sÃ³ dor de cabeÃ§a e '\n",
      "            'enrolaÃ§Ã£o',\n",
      " 'geo_location': {'lat': '-23.5975251', 'long': '-46.6025457'},\n",
      " 'id_review': 'ChZDSUhNMG9nS0VJQ0FnSUMwektQa2NREAEaFk9aOG9qT2kyek1EcTlpUmVybVpJb0E',\n",
      " 'issue': 'business_issue',\n",
      " 'n_photo_user': 0,\n",
      " 'n_review_user': 0,\n",
      " 'rating': 2.0,\n",
      " 'relative_date': '6 meses atrÃ¡s',\n",
      " 'retrieval_date': '2020-04-15T05:33:55Z',\n",
      " 'store': 'magazine-luiza',\n",
      " 'tags': ['business_issues_Delivery'],\n",
      " 'url_user': 'https://www.google.com/maps/contrib/115219057669394551825?hl=pt-BR',\n",
      " 'username': 'Mari Carraro'}\n",
      "{'caption': 'PÃ©ssimo atendimento e falta de variedade de produtos',\n",
      " 'geo_location': {'lat': '-23.5975251', 'long': '-46.6025457'},\n",
      " 'id_review': 'ChdDSUhNMG9nS0VJQ0FnSUMwaExDM3ZBRRABGhZsV3VMZVBMeWNyRHVmWUJGMVBISlpR',\n",
      " 'issue': 'business_issue',\n",
      " 'n_photo_user': 0,\n",
      " 'n_review_user': 0,\n",
      " 'rating': 1.0,\n",
      " 'relative_date': '7 meses atrÃ¡s',\n",
      " 'retrieval_date': '2020-04-15T05:34:02Z',\n",
      " 'store': 'magazine-luiza',\n",
      " 'tags': ['business_issues_Maintenance',\n",
      "          'business_issues_Customer Services',\n",
      "          'business_issues_Delivery'],\n",
      " 'url_user': 'https://www.google.com/maps/contrib/106780407746868145816?hl=pt-BR',\n",
      " 'username': 'Marcia de Lemos Berthoud'}\n",
      "{'caption': 'O vendedor quis me dizer que a loja fÃ­sica Ã© segura e a loja '\n",
      "            'virtual nÃ£o era segura',\n",
      " 'geo_location': {'lat': '-23.5975251', 'long': '-46.6025457'},\n",
      " 'id_review': 'ChZDSUhNMG9nS0VJQ0FnSURVMzVhdEVnEAEaFjRFX25jODdQMlFkMU9mUU5JQk8yV3c',\n",
      " 'issue': 'business_issue',\n",
      " 'n_photo_user': 0,\n",
      " 'n_review_user': 5,\n",
      " 'rating': 1.0,\n",
      " 'relative_date': '7 meses atrÃ¡s',\n",
      " 'retrieval_date': '2020-04-15T05:34:02Z',\n",
      " 'store': 'magazine-luiza',\n",
      " 'tags': ['business_issues_Maintenance', 'business_issues_Customer Services'],\n",
      " 'url_user': 'https://www.google.com/maps/contrib/114450924304017434331?hl=pt-BR',\n",
      " 'username': 'Denis FranÃ§a'}\n",
      "{'caption': 'Desorganizado, atendimento forÃ§ado, caixa e crediÃ¡rio no mesmo '\n",
      "            'local e com poucos atendentes. Ã‰ desagradÃ¡vel ir atÃ© esse local.',\n",
      " 'geo_location': {'lat': '-23.5975251', 'long': '-46.6025457'},\n",
      " 'id_review': 'ChZDSUhNMG9nS0VJQ0FnSURVeUpLRFhnEAEaFlFKRElmTFpwdG5pYlBlczBSWUhVVnc',\n",
      " 'issue': 'product_issue',\n",
      " 'n_photo_user': 0,\n",
      " 'n_review_user': 0,\n",
      " 'rating': 1.0,\n",
      " 'relative_date': '8 meses atrÃ¡s',\n",
      " 'retrieval_date': '2020-04-15T05:34:25Z',\n",
      " 'store': 'magazine-luiza',\n",
      " 'tags': ['product_issues_Quality', 'product_issues_Damaged'],\n",
      " 'url_user': 'https://www.google.com/maps/contrib/108256896680247930485?hl=pt-BR',\n",
      " 'username': 'Anderson Martins'}\n",
      "{'caption': 'Cobra a montagem e entregar do produto.',\n",
      " 'geo_location': {'lat': '-23.5975251', 'long': '-46.6025457'},\n",
      " 'id_review': 'ChZDSUhNMG9nS0VJQ0FnSUNreWJfWU53EAEaFkF0SElBVWhvTUprVldKSXZXS3A2aXc',\n",
      " 'issue': 'business_issue',\n",
      " 'n_photo_user': 0,\n",
      " 'n_review_user': 0,\n",
      " 'rating': 2.0,\n",
      " 'relative_date': '9 meses atrÃ¡s',\n",
      " 'retrieval_date': '2020-04-15T05:35:02Z',\n",
      " 'store': 'magazine-luiza',\n",
      " 'tags': ['business_issues_Maintenance',\n",
      "          'business_issues_Customer Services',\n",
      "          'business_issues_Delivery'],\n",
      " 'url_user': 'https://www.google.com/maps/contrib/104562112282694145903?hl=pt-BR',\n",
      " 'username': 'Edilma Mauricio da Silva'}\n",
      "{'caption': 'Comprei uma geladeira dia 13/05,so ontem04/06  soube q minha '\n",
      "            'compra foi cancelada, prque eu fui na loja saber pq nao '\n",
      "            'chegava... e o vendedor q me vendeu um seguro sem eu querer, em '\n",
      "            'fim ,o caso sera encerado qdo eu receber a geladeira q paguei a '\n",
      "            'mais, dia 10/06 e receber o reembolso.nao recomendaria mais sendo '\n",
      "            'eu cliente antiga.',\n",
      " 'geo_location': {'lat': '-23.5975251', 'long': '-46.6025457'},\n",
      " 'id_review': 'ChdDSUhNMG9nS0VJQ0FnSUM0am9XQTlRRRABGhZ2OEMtLXNpRnFWSGhTT0J0eEMtRkRR',\n",
      " 'issue': 'business_issue',\n",
      " 'n_photo_user': 0,\n",
      " 'n_review_user': 0,\n",
      " 'rating': 1.0,\n",
      " 'relative_date': '10 meses atrÃ¡s',\n",
      " 'retrieval_date': '2020-04-15T05:35:02Z',\n",
      " 'store': 'magazine-luiza',\n",
      " 'tags': ['business_issues_Payment', 'business_issues_Customer Services'],\n",
      " 'url_user': 'https://www.google.com/maps/contrib/100291565031851291646?hl=pt-BR',\n",
      " 'username': 'Maria Elenilce'}\n",
      "{'caption': 'PÃ©ssimo atendimento! NÃ£o resolvem problemas de trocas. NÃ£o '\n",
      "            'recomendo',\n",
      " 'geo_location': {'lat': '-23.5975251', 'long': '-46.6025457'},\n",
      " 'id_review': 'ChdDSUhNMG9nS0VJQ0FnSURZaTRqZmhnRRABGhZNSk94NDRSSFRiNVYzdjk2Z1NodHF3',\n",
      " 'issue': 'product_issue',\n",
      " 'n_photo_user': 0,\n",
      " 'n_review_user': 15,\n",
      " 'rating': 1.0,\n",
      " 'relative_date': '10 meses atrÃ¡s',\n",
      " 'retrieval_date': '2020-04-15T05:35:18Z',\n",
      " 'store': 'magazine-luiza',\n",
      " 'tags': ['product_issues_Quality', 'product_issues_Damaged'],\n",
      " 'url_user': 'https://www.google.com/maps/contrib/111780696239185178681?hl=pt-BR',\n",
      " 'username': 'Adriana Paiva'}\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for r in neg_reviews:\n",
    "    if len(r[\"caption\"]) > 20:\n",
    "        pprint(r)\n",
    "        i+=1\n",
    "    if i ==10:\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"GB_tagged_final-0501.jsonl\", \"w\", encoding=\"utf-8\") as js:\n",
    "    for r in neg_reviews:\n",
    "        js.write(json.dumps(r) + \"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
